{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install necessary packages\n!pip install transformers datasets sacrebleu sentencepiece evaluate peft matplotlib seaborn accelerate bitsandbytes\n\n# Import necessary libraries\nimport os\nimport torch\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time  # For tracking time\nfrom datasets import load_dataset\nfrom transformers import (\n    LlamaForCausalLM,\n    LlamaTokenizer,\n    DataCollatorForSeq2Seq,\n    AdamW,\n    get_scheduler\n)\nfrom peft import get_peft_model, prepare_model_for_kbit_training, LoraConfig, TaskType\nfrom evaluate import load as load_metric\nfrom torch.utils.data import DataLoader\nfrom accelerate import Accelerator\nimport bitsandbytes as bnb\n\n# Start timer for total execution time\nstart_time = time.time()\n\n# Set environment variable to disable parallelism in tokenizers\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n# Initialize accelerator for multi-GPU and optimized performance\naccelerator = Accelerator()\n\n# Check if GPUs are available and set device\ndevice = accelerator.device\nprint(f\"Using device: {device}\")\n\n# Set seed for reproducibility\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\n\n# Track time for dataset loading\nload_dataset_start = time.time()\n\n# Load the dataset\ndataset = load_dataset('findnitai/english-to-hinglish')\n\n# Reduce the dataset size to 30%\nsample_fraction = 0.3\ndataset = dataset['train'].shuffle(seed=seed).select(range(int(len(dataset['train']) * sample_fraction)))\n\n# Split the dataset into train, validation, and test sets\ntrain_test_split = dataset.train_test_split(test_size=0.2, seed=seed)\ntest_valid_split = train_test_split['test'].train_test_split(test_size=0.5, seed=seed)\ntrain_dataset = train_test_split['train']\neval_dataset = test_valid_split['test']\ntest_dataset = test_valid_split['train']\n\n# Print dataset sizes\nprint(f\"Training set size: {len(train_dataset)}\")\nprint(f\"Validation set size: {len(eval_dataset)}\")\nprint(f\"Test set size: {len(test_dataset)}\")\n\n# Dataset loading time\nprint(f\"Dataset loaded in {time.time() - load_dataset_start:.2f} seconds\")\n\n# Define your Hugging Face token\nhf_token = \"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\"\n\n# Initialize tokenizer and model for English-to-Hinglish translation\nmodel_name = \"meta-llama/Llama-2-7b-hf\"\ntokenizer = LlamaTokenizer.from_pretrained(model_name, token=hf_token)\nmodel = LlamaForCausalLM.from_pretrained(model_name, load_in_4bit=True, device_map=\"auto\", quantization_config=bnb.QuantizationConfig(bits=4))\n\n# Prepare model for 4-bit training\nmodel = prepare_model_for_kbit_training(model)\n\n# Define LoRA config with target modules for translation improvement\nlora_config = LoraConfig(\n    task_type=TaskType.SEQ_2_SEQ_LM,\n    inference_mode=False,\n    r=8,\n    lora_alpha=32,\n    lora_dropout=0.1,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\", \"fc1\", \"fc2\"]\n)\nmodel = get_peft_model(model, lora_config)\n\n# Preprocessing function for English-to-Hinglish with max_length set to 32\ndef preprocess_function(examples):\n    inputs = [ex['en'] for ex in examples['translation']]\n    targets = [ex['hi_ng'] for ex in examples['translation']]\n    model_inputs = tokenizer(inputs, max_length=32, padding=\"max_length\", truncation=True)\n    labels = tokenizer(targets, max_length=32, padding=\"max_length\", truncation=True)\n    labels[\"input_ids\"] = [\n        [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n    ]\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\n# Apply preprocessing to datasets and track preprocessing time\npreprocess_start = time.time()\ntrain_dataset = train_dataset.map(preprocess_function, batched=True, remove_columns=[\"translation\"])\neval_dataset = eval_dataset.map(preprocess_function, batched=True, remove_columns=[\"translation\"])\ntest_dataset = test_dataset.map(preprocess_function, batched=True, remove_columns=[\"translation\"])\nprint(f\"Data preprocessed in {time.time() - preprocess_start:.2f} seconds\")\n\n# Data collator\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding=True)\n\n# Dataloaders\ntrain_dataloader = DataLoader(train_dataset, shuffle=True, collate_fn=data_collator, batch_size=32)\neval_dataloader = DataLoader(eval_dataset, collate_fn=data_collator, batch_size=32)\n\n# Optimizer and Scheduler\noptimizer = AdamW(model.parameters(), lr=3e-5)\nnum_training_steps = len(train_dataloader) * 3  # 3 epochs\nlr_scheduler = get_scheduler(\n    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n)\n\n# Metric\nmetric = load_metric('sacrebleu')\n\n# Prepare model, optimizer, and dataloaders with `accelerate`\nmodel, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n    model, optimizer, train_dataloader, eval_dataloader\n)\n\n# Variables to store training and validation results\ntraining_losses = []\nvalidation_bleu_scores = []\n\n# Custom training loop with max_length set to 32\nprint(\"Starting training...\")\nnum_epochs = 3\ntrain_start = time.time()\nfor epoch in range(num_epochs):\n    model.train()\n    epoch_loss = 0.0\n    epoch_start = time.time()\n    for step, batch in enumerate(train_dataloader):\n        with accelerator.accumulate(model):\n            outputs = model(**batch)\n            loss = outputs.loss.mean()\n            epoch_loss += loss.item()\n\n            # Backward pass\n            accelerator.backward(loss)\n            optimizer.step()\n            lr_scheduler.step()\n            optimizer.zero_grad()\n\n        # Logging every 300 steps\n        if step % 300 == 0:\n            avg_loss = epoch_loss / (step + 1)\n            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{step}/{len(train_dataloader)}], Loss: {avg_loss:.4f}\")\n\n    training_losses.append(epoch_loss / len(train_dataloader))\n    print(f\"Epoch [{epoch+1}/{num_epochs}] completed in {time.time() - epoch_start:.2f} seconds\")\n\n    # Validation\n    validation_start = time.time()\n    model.eval()\n    all_preds = []\n    all_labels = []\n    for batch in eval_dataloader:\n        with torch.no_grad():\n            outputs = accelerator.unwrap_model(model).generate(\n                input_ids=batch[\"input_ids\"],\n                attention_mask=batch[\"attention_mask\"],\n                max_length=32,\n                num_beams=4\n            )\n        labels = batch[\"labels\"]\n        labels = np.where(labels.cpu().numpy() != -100, labels.cpu().numpy(), tokenizer.pad_token_id)\n        all_preds.extend(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n        all_labels.extend(tokenizer.batch_decode(labels, skip_special_tokens=True))\n    result = metric.compute(predictions=all_preds, references=[[label] for label in all_labels])\n    validation_bleu_scores.append(result['score'])\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Validation BLEU Score: {result['score']:.2f}\")\n    print(f\"Validation completed in {time.time() - validation_start:.2f} seconds\")\n\nprint(f\"Training completed in {time.time() - train_start:.2f} seconds\")\n\n# Plot training loss and validation BLEU score\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.plot(range(1, num_epochs + 1), training_losses, marker='o', label=\"Training Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training Loss over Epochs\")\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(range(1, num_epochs + 1), validation_bleu_scores, marker='o', color='orange', label=\"Validation BLEU Score\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"BLEU Score\")\nplt.title(\"Validation BLEU Score over Epochs\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n# Save model in Hugging Face format\nmodel_dir = \"./llama2_eng_hing\"\naccelerator.wait_for_everyone()\nunwrapped_model = accelerator.unwrap_model(model)\nunwrapped_model.save_pretrained(model_dir)\ntokenizer.save_pretrained(model_dir)\n\n# Save model as .pt file\ntorch.save(unwrapped_model.state_dict(), \"llama2_eng_hing.pt\")\nprint(\"Model saved as 'llama2_eng_hing.pt'\")\n\n# Test Evaluation and BLEU Score Distribution\nprint(\"Evaluating on the test set...\")\ntest_start = time.time()\ntest_dataloader = DataLoader(test_dataset, collate_fn=data_collator, batch_size=32)\nmodel.eval()\nall_preds = []\nall_labels = []\nindividual_bleu_scores = []\nfor batch in test_dataloader:\n    with torch.no_grad():\n        outputs = accelerator.unwrap_model(model).generate(\n            input_ids=batch[\"input_ids\"],\n            attention_mask=batch[\"attention_mask\"],\n            max_length=32,\n            num_beams=4\n        )\n    labels = batch[\"labels\"]\n    labels = np.where(labels.cpu().numpy() != -100, labels.cpu().numpy(), tokenizer.pad_token_id)\n    all_preds.extend(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n    all_labels.extend(tokenizer.batch_decode(labels, skip_special_tokens=True))\n\n    # Calculate individual BLEU scores for each prediction\n    for pred, label in zip(all_preds, all_labels):\n        bleu = metric.compute(predictions=[pred], references=[[label]])\n        individual_bleu_scores.append(bleu[\"score\"])\n\nprint(f\"Test evaluation completed in {time.time() - test_start:.2f} seconds\")\n\n# Overall BLEU score on the test set\ntest_result = metric.compute(predictions=all_preds, references=[[label] for label in all_labels])\nprint(f\"Test BLEU Score: {test_result['score']:.2f}\")\n\n# Plot BLEU score distribution with density curve and mean line\nplt.figure(figsize=(10, 6))\nsns.histplot(individual_bleu_scores, bins=20, kde=True, color='skyblue', edgecolor='black', alpha=0.7)\nmean_bleu = np.mean(individual_bleu_scores)\nplt.axvline(mean_bleu, color='red', linestyle='--', label=f'Mean BLEU Score: {mean_bleu:.2f}')\nplt.title(\"BLEU Score Distribution on Test Set\")\nplt.xlabel(\"BLEU Score\")\nplt.ylabel(\"Frequency\")\nplt.legend()\nplt.show()\n\n# Display sample translations from test set\ndef display_sample_translations(num_samples=5):\n    sampled_indices = random.sample(range(len(test_dataset)), num_samples)\n    for idx in sampled_indices:\n        input_text = test_dataset[idx]['translation']['en']\n        reference_text = test_dataset[idx]['translation']['hi_ng']\n        input_ids = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=32).input_ids.to(device)\n        with torch.no_grad():\n            generated_tokens = accelerator.unwrap_model(model).generate(\n                input_ids, max_length=32, num_beams=4\n            )\n        predicted_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n\n        print(\"\\nSample Translation:\")\n        print(f\"Input (English): {input_text}\")\n        print(f\"Reference (Hinglish): {reference_text}\")\n        print(f\"Prediction (Model Output): {predicted_text}\")\n\n# Display 5 sample translations from the test set\ndisplay_sample_translations(num_samples=5)\n\n# Test with a custom sentence\ndef translate_sentence(sentence):\n    input_text = \"translate English to Hinglish: \" + sentence\n    input_ids = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=32).input_ids.to(device)\n    with torch.no_grad():\n        generated_tokens = accelerator.unwrap_model(model).generate(\n            input_ids, max_length=32, num_beams=4\n        )\n    return tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n\ncustom_sentence = \"Where is my bag?\"\ntranslated_sentence = translate_sentence(custom_sentence)\nprint(\"\\nCustom Translation:\")\nprint(f\"Input: {custom_sentence}\")\nprint(f\"Translated Output: {translated_sentence}\")\n\n# Total execution time\nprint(f\"Total execution time: {time.time() - start_time:.2f} seconds\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-12T13:48:02.958284Z","iopub.execute_input":"2024-11-12T13:48:02.958677Z","iopub.status.idle":"2024-11-12T13:48:59.140711Z","shell.execute_reply.started":"2024-11-12T13:48:02.958641Z","shell.execute_reply":"2024-11-12T13:48:59.133078Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.1)\nCollecting sacrebleu\n  Downloading sacrebleu-2.4.3-py3-none-any.whl.metadata (51 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (0.2.0)\nCollecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nCollecting peft\n  Downloading peft-0.13.2-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (3.7.5)\nRequirement already satisfied: seaborn in /opt/conda/lib/python3.10/site-packages (0.12.2)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.34.2)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nCollecting portalocker (from sacrebleu)\n  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (5.3.0)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.4.0)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.2.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (4.53.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.4.5)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (10.3.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading sacrebleu-2.4.3-py3-none-any.whl (103 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.0/104.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading peft-0.13.2-py3-none-any.whl (320 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading portalocker-2.10.1-py3-none-any.whl (18 kB)\nInstalling collected packages: portalocker, sacrebleu, bitsandbytes, peft, evaluate\nSuccessfully installed bitsandbytes-0.44.1 evaluate-0.4.3 peft-0.13.2 portalocker-2.10.1 sacrebleu-2.4.3\nUsing device: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/367 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5f75b8c119b4fbbb795399fe039c769"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"hinglish_upload_v1.json:   0%|          | 0.00/27.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"619d2ab2583c4dbbbb2699287397188e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/189102 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0170bbd89368429a94d356c6fe0cd41b"}},"metadata":{}},{"name":"stdout","text":"Training set size: 45384\nValidation set size: 5673\nTest set size: 5673\nDataset loaded in 2.79 seconds\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3cb3a8170b545b7919c15bc1ef8c796"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a75c7e810eb949739566551c8ffc7bfd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae06a845b0df4013a24f47e18724cb4c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97a95e6d5a07403daba77e7d5011387e"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 76\u001b[0m\n\u001b[1;32m     74\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Llama-2-7b-hf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     75\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m LlamaTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name, token\u001b[38;5;241m=\u001b[39mhf_token)\n\u001b[0;32m---> 76\u001b[0m model \u001b[38;5;241m=\u001b[39m LlamaForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name, load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m, quantization_config\u001b[38;5;241m=\u001b[39m\u001b[43mbnb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mQuantizationConfig\u001b[49m(bits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m))\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Prepare model for 4-bit training\u001b[39;00m\n\u001b[1;32m     79\u001b[0m model \u001b[38;5;241m=\u001b[39m prepare_model_for_kbit_training(model)\n","\u001b[0;31mAttributeError\u001b[0m: module 'bitsandbytes' has no attribute 'QuantizationConfig'"],"ename":"AttributeError","evalue":"module 'bitsandbytes' has no attribute 'QuantizationConfig'","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"# Install necessary packages\n!pip install transformers datasets sacrebleu sentencepiece evaluate peft matplotlib seaborn accelerate bitsandbytes\n\n# Import necessary libraries\nimport os\nimport torch\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time  # For tracking time\nfrom datasets import load_dataset\nfrom transformers import (\n    LlamaForCausalLM,\n    LlamaTokenizer,\n    DataCollatorForSeq2Seq,\n    AdamW,\n    get_scheduler\n)\nfrom peft import get_peft_model, prepare_model_for_kbit_training, LoraConfig, TaskType\nfrom evaluate import load as load_metric\nfrom torch.utils.data import DataLoader\nfrom accelerate import Accelerator\nfrom bitsandbytes.nn import Linear4bit  # Correct quantization module\nimport bitsandbytes as bnb\n\n# Start timer for total execution time\nstart_time = time.time()\n\n# Set environment variable to disable parallelism in tokenizers\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n# Initialize accelerator for multi-GPU and optimized performance\naccelerator = Accelerator()\n\n# Check if GPUs are available and set device\ndevice = accelerator.device\nprint(f\"Using device: {device}\")\n\n# Set seed for reproducibility\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\n\n# Track time for dataset loading\nload_dataset_start = time.time()\n\n# Load the dataset\ndataset = load_dataset('findnitai/english-to-hinglish')\n\n# Reduce the dataset size to 30%\nsample_fraction = 0.3\ndataset = dataset['train'].shuffle(seed=seed).select(range(int(len(dataset['train']) * sample_fraction)))\n\n# Split the dataset into train, validation, and test sets\ntrain_test_split = dataset.train_test_split(test_size=0.2, seed=seed)\ntest_valid_split = train_test_split['test'].train_test_split(test_size=0.5, seed=seed)\ntrain_dataset = train_test_split['train']\neval_dataset = test_valid_split['test']\ntest_dataset = test_valid_split['train']\n\n# Print dataset sizes\nprint(f\"Training set size: {len(train_dataset)}\")\nprint(f\"Validation set size: {len(eval_dataset)}\")\nprint(f\"Test set size: {len(test_dataset)}\")\n\n# Dataset loading time\nprint(f\"Dataset loaded in {time.time() - load_dataset_start:.2f} seconds\")\n\n# Define your Hugging Face token\nhf_token = \"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\"  # **Ensure to keep your token secure!**\n\n# Initialize tokenizer and model for English-to-Hinglish translation\nmodel_name = \"meta-llama/Llama-2-7b-hf\"\ntokenizer = LlamaTokenizer.from_pretrained(model_name, use_fast=True)\n\n# Load the model with 4-bit quantization\nquantization_config = bnb.nn.quantization.Linear4bit(\n    quant_type='nf4',  # NormalFloat4 quantization\n    compute_dtype=torch.float16  # Use float16 for computations\n)\n\nmodel = LlamaForCausalLM.from_pretrained(\n    model_name,\n    load_in_4bit=True,\n    device_map=\"auto\",\n    quantization_config=quantization_config,\n    torch_dtype=torch.float16  # Ensure consistent dtype\n)\n\n# Prepare model for 4-bit training\nmodel = prepare_model_for_kbit_training(model)\n\n# Define LoRA config with appropriate task type and target modules\nlora_config = LoraConfig(\n    task_type=TaskType.CAUSAL_LM,  # Correct task type for causal language models\n    inference_mode=False,\n    r=8,\n    lora_alpha=32,\n    lora_dropout=0.1,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]  # Accurate target modules for LLaMA\n)\n\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()  # Optional: Print trainable parameters\n\n# Preprocessing function for English-to-Hinglish with max_length set to 32\ndef preprocess_function(examples):\n    inputs = [ex['en'] for ex in examples['translation']]\n    targets = [ex['hi_ng'] for ex in examples['translation']]\n    model_inputs = tokenizer(inputs, max_length=32, padding=\"max_length\", truncation=True)\n    labels = tokenizer(targets, max_length=32, padding=\"max_length\", truncation=True)\n    labels[\"input_ids\"] = [\n        [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n    ]\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\n# Apply preprocessing to datasets and track preprocessing time\npreprocess_start = time.time()\ntrain_dataset = train_dataset.map(preprocess_function, batched=True, remove_columns=[\"translation\"])\neval_dataset = eval_dataset.map(preprocess_function, batched=True, remove_columns=[\"translation\"])\ntest_dataset = test_dataset.map(preprocess_function, batched=True, remove_columns=[\"translation\"])\nprint(f\"Data preprocessed in {time.time() - preprocess_start:.2f} seconds\")\n\n# Data collator\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding=True)\n\n# Dataloaders\ntrain_dataloader = DataLoader(train_dataset, shuffle=True, collate_fn=data_collator, batch_size=32)\neval_dataloader = DataLoader(eval_dataset, collate_fn=data_collator, batch_size=32)\n\n# Optimizer and Scheduler\noptimizer = AdamW(model.parameters(), lr=3e-5)\nnum_training_steps = len(train_dataloader) * 3  # 3 epochs\nlr_scheduler = get_scheduler(\n    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n)\n\n# Metric\nmetric = load_metric('sacrebleu')\n\n# Prepare model, optimizer, and dataloaders with `accelerate`\nmodel, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n    model, optimizer, train_dataloader, eval_dataloader\n)\n\n# Variables to store training and validation results\ntraining_losses = []\nvalidation_bleu_scores = []\n\n# Custom training loop with max_length set to 32\nprint(\"Starting training...\")\nnum_epochs = 3\ntrain_start = time.time()\nfor epoch in range(num_epochs):\n    model.train()\n    epoch_loss = 0.0\n    epoch_start = time.time()\n    for step, batch in enumerate(train_dataloader):\n        with accelerator.accumulate(model):\n            outputs = model(**batch)\n            loss = outputs.loss\n            epoch_loss += loss.item()\n\n            # Backward pass\n            accelerator.backward(loss)\n            optimizer.step()\n            lr_scheduler.step()\n            optimizer.zero_grad()\n\n        # Logging every 300 steps\n        if step % 300 == 0:\n            avg_loss = epoch_loss / (step + 1)\n            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{step}/{len(train_dataloader)}], Loss: {avg_loss:.4f}\")\n\n    training_losses.append(epoch_loss / len(train_dataloader))\n    print(f\"Epoch [{epoch+1}/{num_epochs}] completed in {time.time() - epoch_start:.2f} seconds\")\n\n    # Validation\n    validation_start = time.time()\n    model.eval()\n    all_preds = []\n    all_labels = []\n    for batch in eval_dataloader:\n        with torch.no_grad():\n            outputs = accelerator.unwrap_model(model).generate(\n                input_ids=batch[\"input_ids\"],\n                attention_mask=batch[\"attention_mask\"],\n                max_length=32,\n                num_beams=4\n            )\n        labels = batch[\"labels\"]\n        labels = np.where(labels.cpu().numpy() != -100, labels.cpu().numpy(), tokenizer.pad_token_id)\n        decoded_preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n        all_preds.extend(decoded_preds)\n        all_labels.extend(decoded_labels)\n    result = metric.compute(predictions=all_preds, references=[[label] for label in all_labels])\n    validation_bleu_scores.append(result['score'])\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Validation BLEU Score: {result['score']:.2f}\")\n    print(f\"Validation completed in {time.time() - validation_start:.2f} seconds\")\n\nprint(f\"Training completed in {time.time() - train_start:.2f} seconds\")\n\n# Plot training loss and validation BLEU score\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.plot(range(1, num_epochs + 1), training_losses, marker='o', label=\"Training Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training Loss over Epochs\")\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(range(1, num_epochs + 1), validation_bleu_scores, marker='o', color='orange', label=\"Validation BLEU Score\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"BLEU Score\")\nplt.title(\"Validation BLEU Score over Epochs\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n# Save model in Hugging Face format\nmodel_dir = \"./llama2_eng_hing\"\naccelerator.wait_for_everyone()\nunwrapped_model = accelerator.unwrap_model(model)\nunwrapped_model.save_pretrained(model_dir, save_function=accelerator.save)\ntokenizer.save_pretrained(model_dir)\n\n# Save model as .pt file\ntorch.save(unwrapped_model.state_dict(), \"llama2_eng_hing.pt\")\nprint(\"Model saved as 'llama2_eng_hing.pt'\")\n\n# Test Evaluation and BLEU Score Distribution\nprint(\"Evaluating on the test set...\")\ntest_start = time.time()\ntest_dataloader = DataLoader(test_dataset, collate_fn=data_collator, batch_size=32)\nmodel.eval()\nall_preds = []\nall_labels = []\nindividual_bleu_scores = []\nfor batch in test_dataloader:\n    with torch.no_grad():\n        outputs = accelerator.unwrap_model(model).generate(\n            input_ids=batch[\"input_ids\"],\n            attention_mask=batch[\"attention_mask\"],\n            max_length=32,\n            num_beams=4\n        )\n    labels = batch[\"labels\"]\n    labels = np.where(labels.cpu().numpy() != -100, labels.cpu().numpy(), tokenizer.pad_token_id)\n    decoded_preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    all_preds.extend(decoded_preds)\n    all_labels.extend(decoded_labels)\n\n    # Calculate individual BLEU scores for each prediction\n    for pred, label in zip(decoded_preds, decoded_labels):\n        bleu = metric.compute(predictions=[pred], references=[[label]])\n        individual_bleu_scores.append(bleu[\"score\"])\n\nprint(f\"Test evaluation completed in {time.time() - test_start:.2f} seconds\")\n\n# Overall BLEU score on the test set\ntest_result = metric.compute(predictions=all_preds, references=[[label] for label in all_labels])\nprint(f\"Test BLEU Score: {test_result['score']:.2f}\")\n\n# Plot BLEU score distribution with density curve and mean line\nplt.figure(figsize=(10, 6))\nsns.histplot(individual_bleu_scores, bins=20, kde=True, color='skyblue', edgecolor='black', alpha=0.7)\nmean_bleu = np.mean(individual_bleu_scores)\nplt.axvline(mean_bleu, color='red', linestyle='--', label=f'Mean BLEU Score: {mean_bleu:.2f}')\nplt.title(\"BLEU Score Distribution on Test Set\")\nplt.xlabel(\"BLEU Score\")\nplt.ylabel(\"Frequency\")\nplt.legend()\nplt.show()\n\n# Display sample translations from test set\ndef display_sample_translations(num_samples=5):\n    sampled_indices = random.sample(range(len(test_dataset)), num_samples)\n    for idx in sampled_indices:\n        input_text = test_dataset[idx]['translation']['en']\n        reference_text = test_dataset[idx]['translation']['hi_ng']\n        input_ids = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=32).input_ids.to(device)\n        with torch.no_grad():\n            generated_tokens = accelerator.unwrap_model(model).generate(\n                input_ids, max_length=32, num_beams=4\n            )\n        predicted_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n\n        print(\"\\nSample Translation:\")\n        print(f\"Input (English): {input_text}\")\n        print(f\"Reference (Hinglish): {reference_text}\")\n        print(f\"Prediction (Model Output): {predicted_text}\")\n\n# Display 5 sample translations from the test set\ndisplay_sample_translations(num_samples=5)\n\n# Test with a custom sentence\ndef translate_sentence(sentence):\n    input_text = sentence  # Remove \"translate English to Hinglish:\" prefix if not needed\n    input_ids = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=32).input_ids.to(device)\n    with torch.no_grad():\n        generated_tokens = accelerator.unwrap_model(model).generate(\n            input_ids, max_length=32, num_beams=4\n        )\n    return tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n\ncustom_sentence = \"Where is my bag?\"\ntranslated_sentence = translate_sentence(custom_sentence)\nprint(\"\\nCustom Translation:\")\nprint(f\"Input: {custom_sentence}\")\nprint(f\"Translated Output: {translated_sentence}\")\n\n# Total execution time\nprint(f\"Total execution time: {time.time() - start_time:.2f} seconds\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T13:54:15.472856Z","iopub.execute_input":"2024-11-12T13:54:15.473211Z","iopub.status.idle":"2024-11-12T13:54:28.807384Z","shell.execute_reply.started":"2024-11-12T13:54:15.473178Z","shell.execute_reply":"2024-11-12T13:54:28.806065Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.1)\nRequirement already satisfied: sacrebleu in /opt/conda/lib/python3.10/site-packages (2.4.3)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (0.2.0)\nRequirement already satisfied: evaluate in /opt/conda/lib/python3.10/site-packages (0.4.3)\nRequirement already satisfied: peft in /opt/conda/lib/python3.10/site-packages (0.13.2)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (3.7.5)\nRequirement already satisfied: seaborn in /opt/conda/lib/python3.10/site-packages (0.12.2)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.34.2)\nRequirement already satisfied: bitsandbytes in /opt/conda/lib/python3.10/site-packages (0.44.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: portalocker in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (2.10.1)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (5.3.0)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.4.0)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.2.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (4.53.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.4.5)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (10.3.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nUsing device: cuda\nTraining set size: 45384\nValidation set size: 5673\nTest set size: 5673\nDataset loaded in 0.80 seconds\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 79\u001b[0m\n\u001b[1;32m     76\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m LlamaTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name, use_fast\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Load the model with 4-bit quantization\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m quantization_config \u001b[38;5;241m=\u001b[39m \u001b[43mbnb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantization\u001b[49m\u001b[38;5;241m.\u001b[39mLinear4bit(\n\u001b[1;32m     80\u001b[0m     quant_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# NormalFloat4 quantization\u001b[39;00m\n\u001b[1;32m     81\u001b[0m     compute_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16  \u001b[38;5;66;03m# Use float16 for computations\u001b[39;00m\n\u001b[1;32m     82\u001b[0m )\n\u001b[1;32m     84\u001b[0m model \u001b[38;5;241m=\u001b[39m LlamaForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     85\u001b[0m     model_name,\n\u001b[1;32m     86\u001b[0m     load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16  \u001b[38;5;66;03m# Ensure consistent dtype\u001b[39;00m\n\u001b[1;32m     90\u001b[0m )\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# Prepare model for 4-bit training\u001b[39;00m\n","\u001b[0;31mAttributeError\u001b[0m: module 'bitsandbytes.nn' has no attribute 'quantization'"],"ename":"AttributeError","evalue":"module 'bitsandbytes.nn' has no attribute 'quantization'","output_type":"error"}],"execution_count":3},{"cell_type":"code","source":"!export HF_TOKEN=\"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\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T14:00:01.150548Z","iopub.execute_input":"2024-11-12T14:00:01.150964Z","iopub.status.idle":"2024-11-12T14:00:02.162927Z","shell.execute_reply.started":"2024-11-12T14:00:01.150925Z","shell.execute_reply":"2024-11-12T14:00:02.161669Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Install necessary packages\n!pip install --upgrade transformers datasets sacrebleu sentencepiece evaluate peft matplotlib seaborn accelerate bitsandbytes>=0.39.0\n\n# Import necessary libraries\nimport os\nimport torch\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time  # For tracking time\nfrom datasets import load_dataset\nfrom transformers import (\n    LlamaForCausalLM,\n    LlamaTokenizer,\n    DataCollatorForSeq2Seq,\n    AdamW,\n    get_scheduler,\n    BitsAndBytesConfig\n)\nfrom peft import get_peft_model, prepare_model_for_kbit_training, LoraConfig, TaskType\nfrom evaluate import load as load_metric\nfrom torch.utils.data import DataLoader\nfrom accelerate import Accelerator\n\n# Start timer for total execution time\nstart_time = time.time()\n\n# Set environment variable to disable parallelism in tokenizers\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n# Initialize accelerator for multi-GPU and optimized performance\naccelerator = Accelerator()\n\n# Check if GPUs are available and set device\ndevice = accelerator.device\nprint(f\"Using device: {device}\")\n\n# Set seed for reproducibility\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(seed)\n\n# Track time for dataset loading\nload_dataset_start = time.time()\n\n# Load the dataset\ndataset = load_dataset('findnitai/english-to-hinglish')\n\n# Reduce the dataset size to 30% for faster experimentation\nsample_fraction = 0.3\ndataset = dataset['train'].shuffle(seed=seed).select(range(int(len(dataset['train']) * sample_fraction)))\n\n# Split the dataset into train, validation, and test sets\ntrain_test_split = dataset.train_test_split(test_size=0.2, seed=seed)\ntest_valid_split = train_test_split['test'].train_test_split(test_size=0.5, seed=seed)\ntrain_dataset = train_test_split['train']\neval_dataset = test_valid_split['test']\ntest_dataset = test_valid_split['train']\n\n# Print dataset sizes\nprint(f\"Training set size: {len(train_dataset)}\")\nprint(f\"Validation set size: {len(eval_dataset)}\")\nprint(f\"Test set size: {len(test_dataset)}\")\n\n# Dataset loading time\nprint(f\"Dataset loaded in {time.time() - load_dataset_start:.2f} seconds\")\n\n# Define your Hugging Face token securely\n# ⚠️ WARNING: Hardcoding tokens is insecure. Ensure that this script is kept private and not shared publicly.\nhf_token = \"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\"  # Replace with your actual Hugging Face token\n\n# Initialize tokenizer with authentication\nmodel_name = \"meta-llama/Llama-2-7b-hf\"\ntokenizer = LlamaTokenizer.from_pretrained(model_name, use_fast=True, use_auth_token=hf_token)\n\n# Define padding token if not already defined\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n    print(\"Pad token not found. Setting pad_token to eos_token.\")\n\n# Update model config with pad_token_id\n# (This step is optional but ensures consistency)\npad_token_id = tokenizer.pad_token_id\n# Some models may require this to be explicitly set\n# Depending on the model, this might not be necessary\n# For LLaMA, it's generally sufficient to set the pad_token in the tokenizer\n\n# Define BitsAndBytesConfig for 4-bit quantization\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",            # NormalFloat4 quantization\n    bnb_4bit_compute_dtype=torch.float16, # Use float16 for computations\n)\n\n# Load the model with 4-bit quantization and authenticate using the token\nmodel = LlamaForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=quantization_config,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,  # Ensure consistent dtype\n    use_auth_token=hf_token    # Authenticate to access the gated repo\n)\n\n# Prepare model for 4-bit training (necessary for QLoRA)\nmodel = prepare_model_for_kbit_training(model)\n\n# Enable gradient checkpointing for memory optimization (optional)\nmodel.gradient_checkpointing_enable()\n\n# Define LoRA config with appropriate task type and target modules\nlora_config = LoraConfig(\n    task_type=TaskType.CAUSAL_LM,  # Correct task type for causal language models\n    inference_mode=False,\n    r=8,\n    lora_alpha=32,\n    lora_dropout=0.1,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]  # Accurate target modules for LLaMA\n)\n\n# Apply LoRA to the model\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()  # Optional: Print trainable parameters\n\n# Preprocessing function for English-to-Hinglish with max_length set to 32\ndef preprocess_function(examples):\n    inputs = [ex['en'] for ex in examples['translation']]\n    targets = [ex['hi_ng'] for ex in examples['translation']]\n    model_inputs = tokenizer(inputs, max_length=32, padding=\"max_length\", truncation=True)\n    labels = tokenizer(targets, max_length=32, padding=\"max_length\", truncation=True)\n    labels[\"input_ids\"] = [\n        [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n    ]\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\n# Apply preprocessing to datasets and track preprocessing time\npreprocess_start = time.time()\ntrain_dataset = train_dataset.map(preprocess_function, batched=True, remove_columns=[\"translation\"])\neval_dataset = eval_dataset.map(preprocess_function, batched=True, remove_columns=[\"translation\"])\ntest_dataset = test_dataset.map(preprocess_function, batched=True, remove_columns=[\"translation\"])\nprint(f\"Data preprocessed in {time.time() - preprocess_start:.2f} seconds\")\n\n# Data collator\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding=True)\n\n# Dataloaders\ntrain_dataloader = DataLoader(train_dataset, shuffle=True, collate_fn=data_collator, batch_size=32)\neval_dataloader = DataLoader(eval_dataset, collate_fn=data_collator, batch_size=32)\n\n# Optimizer and Scheduler\noptimizer = AdamW(model.parameters(), lr=3e-5)\nnum_training_steps = len(train_dataloader) * 3  # 3 epochs\nlr_scheduler = get_scheduler(\n    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n)\n\n# Metric\nmetric = load_metric('sacrebleu')\n\n# Prepare model, optimizer, and dataloaders with `accelerate`\nmodel, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n    model, optimizer, train_dataloader, eval_dataloader\n)\n\n# Variables to store training and validation results\ntraining_losses = []\nvalidation_bleu_scores = []\n\n# Custom training loop with max_length set to 32\nprint(\"Starting training...\")\nnum_epochs = 3\ntrain_start = time.time()\nfor epoch in range(num_epochs):\n    model.train()\n    epoch_loss = 0.0\n    epoch_start = time.time()\n    for step, batch in enumerate(train_dataloader):\n        with accelerator.accumulate(model):\n            outputs = model(**batch)\n            loss = outputs.loss\n            epoch_loss += loss.item()\n\n            # Backward pass\n            accelerator.backward(loss)\n            optimizer.step()\n            lr_scheduler.step()\n            optimizer.zero_grad()\n\n        # Logging every 300 steps\n        if step % 300 == 0 and step != 0:\n            avg_loss = epoch_loss / (step + 1)\n            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{step}/{len(train_dataloader)}], Loss: {avg_loss:.4f}\")\n\n    training_losses.append(epoch_loss / len(train_dataloader))\n    print(f\"Epoch [{epoch+1}/{num_epochs}] completed in {time.time() - epoch_start:.2f} seconds\")\n\n    # Validation\n    validation_start = time.time()\n    model.eval()\n    all_preds = []\n    all_labels = []\n    for batch in eval_dataloader:\n        with torch.no_grad():\n            outputs = accelerator.unwrap_model(model).generate(\n                input_ids=batch[\"input_ids\"],\n                attention_mask=batch[\"attention_mask\"],\n                max_length=32,\n                num_beams=4\n            )\n        labels = batch[\"labels\"]\n        labels = np.where(labels.cpu().numpy() != -100, labels.cpu().numpy(), tokenizer.pad_token_id)\n        decoded_preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n        all_preds.extend(decoded_preds)\n        all_labels.extend(decoded_labels)\n    result = metric.compute(predictions=all_preds, references=[[label] for label in all_labels])\n    validation_bleu_scores.append(result['score'])\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Validation BLEU Score: {result['score']:.2f}\")\n    print(f\"Validation completed in {time.time() - validation_start:.2f} seconds\")\n\nprint(f\"Training completed in {time.time() - train_start:.2f} seconds\")\n\n# Plot training loss and validation BLEU score\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.plot(range(1, num_epochs + 1), training_losses, marker='o', label=\"Training Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training Loss over Epochs\")\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(range(1, num_epochs + 1), validation_bleu_scores, marker='o', color='orange', label=\"Validation BLEU Score\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"BLEU Score\")\nplt.title(\"Validation BLEU Score over Epochs\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n# Save model in Hugging Face format\nmodel_dir = \"./llama2_eng_hing\"\naccelerator.wait_for_everyone()\nunwrapped_model = accelerator.unwrap_model(model)\nunwrapped_model.save_pretrained(model_dir, save_function=accelerator.save)\ntokenizer.save_pretrained(model_dir)\n\n# Save model as .pt file\ntorch.save(unwrapped_model.state_dict(), \"llama2_eng_hing.pt\")\nprint(\"Model saved as 'llama2_eng_hing.pt'\")\n\n# Test Evaluation and BLEU Score Distribution\nprint(\"Evaluating on the test set...\")\ntest_start = time.time()\ntest_dataloader = DataLoader(test_dataset, collate_fn=data_collator, batch_size=32)\nmodel.eval()\nall_preds = []\nall_labels = []\nindividual_bleu_scores = []\nfor batch in test_dataloader:\n    with torch.no_grad():\n        outputs = accelerator.unwrap_model(model).generate(\n            input_ids=batch[\"input_ids\"],\n            attention_mask=batch[\"attention_mask\"],\n            max_length=32,\n            num_beams=4\n        )\n    labels = batch[\"labels\"]\n    labels = np.where(labels.cpu().numpy() != -100, labels.cpu().numpy(), tokenizer.pad_token_id)\n    decoded_preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    all_preds.extend(decoded_preds)\n    all_labels.extend(decoded_labels)\n\n    # Calculate individual BLEU scores for each prediction\n    for pred, label in zip(decoded_preds, decoded_labels):\n        bleu = metric.compute(predictions=[pred], references=[[label]])\n        individual_bleu_scores.append(bleu[\"score\"])\n\nprint(f\"Test evaluation completed in {time.time() - test_start:.2f} seconds\")\n\n# Overall BLEU score on the test set\ntest_result = metric.compute(predictions=all_preds, references=[[label] for label in all_labels])\nprint(f\"Test BLEU Score: {test_result['score']:.2f}\")\n\n# Plot BLEU score distribution with density curve and mean line\nplt.figure(figsize=(10, 6))\nsns.histplot(individual_bleu_scores, bins=20, kde=True, color='skyblue', edgecolor='black', alpha=0.7)\nmean_bleu = np.mean(individual_bleu_scores)\nplt.axvline(mean_bleu, color='red', linestyle='--', label=f'Mean BLEU Score: {mean_bleu:.2f}')\nplt.title(\"BLEU Score Distribution on Test Set\")\nplt.xlabel(\"BLEU Score\")\nplt.ylabel(\"Frequency\")\nplt.legend()\nplt.show()\n\n# Display sample translations from test set\ndef display_sample_translations(num_samples=5):\n    sampled_indices = random.sample(range(len(test_dataset)), num_samples)\n    for idx in sampled_indices:\n        input_text = test_dataset[idx]['translation']['en']\n        reference_text = test_dataset[idx]['translation']['hi_ng']\n        input_ids = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=32).input_ids.to(device)\n        with torch.no_grad():\n            generated_tokens = accelerator.unwrap_model(model).generate(\n                input_ids, max_length=32, num_beams=4\n            )\n        predicted_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n\n        print(\"\\nSample Translation:\")\n        print(f\"Input (English): {input_text}\")\n        print(f\"Reference (Hinglish): {reference_text}\")\n        print(f\"Prediction (Model Output): {predicted_text}\")\n\n# Display 5 sample translations from the test set\ndisplay_sample_translations(num_samples=5)\n\n# Test with a custom sentence\ndef translate_sentence(sentence):\n    input_text = sentence  # Removed \"translate English to Hinglish:\" prefix for consistency\n    input_ids = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=32).input_ids.to(device)\n    with torch.no_grad():\n        generated_tokens = accelerator.unwrap_model(model).generate(\n            input_ids, max_length=32, num_beams=4\n        )\n    return tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n\ncustom_sentence = \"Where is my bag?\"\ntranslated_sentence = translate_sentence(custom_sentence)\nprint(\"\\nCustom Translation:\")\nprint(f\"Input: {custom_sentence}\")\nprint(f\"Translated Output: {translated_sentence}\")\n\n# Total execution time\nprint(f\"Total execution time: {time.time() - start_time:.2f} seconds\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T14:14:40.561585Z","iopub.execute_input":"2024-11-12T14:14:40.561997Z","iopub.status.idle":"2024-11-12T14:17:22.348715Z","shell.execute_reply.started":"2024-11-12T14:14:40.561959Z","shell.execute_reply":"2024-11-12T14:17:22.343235Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Using device: cuda\nTraining set size: 45384\nValidation set size: 5673\nTest set size: 5673\nDataset loaded in 0.77 seconds\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2080: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  )\n","output_type":"stream"},{"name":"stdout","text":"Pad token not found. Setting pad_token to eos_token.\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:3274: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1658289d98f4675ac0c43179322c2c2"}},"metadata":{}},{"name":"stdout","text":"trainable params: 8,388,608 || all params: 6,746,804,224 || trainable%: 0.1243\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/45384 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b486131a8ba842c8a2cd7b7b9ec9097c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5673 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7bb635a99de4ad9bc3dc30371b2bc68"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5673 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80d8c1f1faeb40cab2f97f64ab4ba016"}},"metadata":{}},{"name":"stdout","text":"Data preprocessed in 20.80 seconds\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/8.15k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"541af63e76d44c88b287c1d16ed141bd"}},"metadata":{}},{"name":"stderr","text":"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n","output_type":"stream"},{"name":"stdout","text":"Starting training...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 188\u001b[0m\n\u001b[1;32m    185\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[0;32m--> 188\u001b[0m \u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    190\u001b[0m lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:2196\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2194\u001b[0m \u001b[38;5;66;03m# We try to find the optimizer associated with `scheduler`, the default is the full list.\u001b[39;00m\n\u001b[1;32m   2195\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizers\n\u001b[0;32m-> 2196\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m opt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizers:\n\u001b[1;32m   2197\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(scheduler, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m opt\u001b[38;5;241m.\u001b[39moptimizer:\n\u001b[1;32m   2198\u001b[0m         optimizer \u001b[38;5;241m=\u001b[39m opt\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":12},{"cell_type":"code","source":"# Install necessary packages\n!pip install --upgrade transformers datasets sacrebleu sentencepiece evaluate peft matplotlib seaborn accelerate bitsandbytes>=0.39.0\n\n# Import necessary libraries\nimport os\nimport torch\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time  # For tracking time\nfrom datasets import load_dataset\nfrom transformers import (\n    LlamaForCausalLM,\n    LlamaTokenizerFast,  # Use the fast tokenizer\n    DataCollatorForSeq2Seq,\n    AdamW,\n    get_scheduler\n)\nfrom peft import get_peft_model, prepare_model_for_kbit_training, LoraConfig, TaskType\nfrom evaluate import load as load_metric\nfrom torch.utils.data import DataLoader\nfrom accelerate import Accelerator\n\n# Start timer for total execution time\nstart_time = time.time()\n\n# Set environment variable to disable parallelism in tokenizers\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n# Initialize accelerator for multi-GPU and optimized performance\naccelerator = Accelerator()\n\n# Check if GPUs are available and set device\ndevice = accelerator.device\nprint(f\"Using device: {device}\")\n\n# Set seed for reproducibility\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(seed)\n\n# Track time for dataset loading\nload_dataset_start = time.time()\n\n# Load the dataset\ndataset = load_dataset('findnitai/english-to-hinglish')\n\n# Reduce the dataset size to 30% for faster experimentation\nsample_fraction = 0.3\ndataset = dataset['train'].shuffle(seed=seed).select(range(int(len(dataset['train']) * sample_fraction)))\n\n# Split the dataset into train, validation, and test sets\ntrain_test_split = dataset.train_test_split(test_size=0.2, seed=seed)\ntest_valid_split = train_test_split['test'].train_test_split(test_size=0.5, seed=seed)\ntrain_dataset = train_test_split['train']\neval_dataset = test_valid_split['test']\ntest_dataset = test_valid_split['train']\n\n# Print dataset sizes\nprint(f\"Training set size: {len(train_dataset)}\")\nprint(f\"Validation set size: {len(eval_dataset)}\")\nprint(f\"Test set size: {len(test_dataset)}\")\n\n# Dataset loading time\nprint(f\"Dataset loaded in {time.time() - load_dataset_start:.2f} seconds\")\n\n# Define your Hugging Face token securely\n# ⚠️ WARNING: Hardcoding tokens is insecure. Ensure that this script is kept private and not shared publicly.\nhf_token = \"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\"  # Replace with your actual Hugging Face token\n\n# Initialize tokenizer with authentication\nmodel_name = \"unsloth/Llama-3.2-1B-bnb-4bit\"\ntokenizer = LlamaTokenizerFast.from_pretrained(model_name, token=hf_token, legacy=False)\n\n# Define padding token if not already defined\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n    print(\"Pad token not found. Setting pad_token to eos_token.\")\n\n# Optional: Update model config with pad_token_id\n# For many models, setting pad_token in the tokenizer is sufficient\npad_token_id = tokenizer.pad_token_id\n\n# Load the model with authentication\nmodel = LlamaForCausalLM.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,  # Ensure consistent dtype\n    token=hf_token    # Replaced use_auth_token with token\n)\n\n# Prepare model for k-bit training (necessary for QLoRA)\nmodel = prepare_model_for_kbit_training(model)\n\n# Enable gradient checkpointing for memory optimization (optional)\nmodel.gradient_checkpointing_enable()\n\n# Define LoRA config with appropriate task type and target modules\nlora_config = LoraConfig(\n    task_type=TaskType.CAUSAL_LM,  # Correct task type for causal language models\n    inference_mode=False,\n    r=8,\n    lora_alpha=32,\n    lora_dropout=0.1,\n    target_modules=[\"gate_proj\", \"up_proj\", \"down_proj\"]  # Adjust based on the model architecture\n)\n\n# Apply LoRA to the model\ntry:\n    model = get_peft_model(model, lora_config)\n    model.print_trainable_parameters()  # Optional: Print trainable parameters\nexcept ValueError as e:\n    print(f\"Error during LoRA injection: {e}\")\n    print(\"Ensure that the target modules are correctly specified and supported.\")\n    raise e\n\n# Preprocessing function for English-to-Hinglish with max_length set to 32\ndef preprocess_function(examples):\n    inputs = [ex['en'] for ex in examples['translation']]\n    targets = [ex['hi_ng'] for ex in examples['translation']]\n    model_inputs = tokenizer(inputs, max_length=32, padding=\"max_length\", truncation=True)\n    labels = tokenizer(targets, max_length=32, padding=\"max_length\", truncation=True)\n    labels[\"input_ids\"] = [\n        [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n    ]\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\n# Apply preprocessing to datasets and track preprocessing time\npreprocess_start = time.time()\ntrain_dataset = train_dataset.map(preprocess_function, batched=True, remove_columns=[\"translation\"])\neval_dataset = eval_dataset.map(preprocess_function, batched=True, remove_columns=[\"translation\"])\ntest_dataset = test_dataset.map(preprocess_function, batched=True, remove_columns=[\"translation\"])\nprint(f\"Data preprocessed in {time.time() - preprocess_start:.2f} seconds\")\n\n# Data collator\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding=True)\n\n# Dataloaders\ntrain_dataloader = DataLoader(train_dataset, shuffle=True, collate_fn=data_collator, batch_size=32)\neval_dataloader = DataLoader(eval_dataset, collate_fn=data_collator, batch_size=32)\n\n# Optimizer and Scheduler\noptimizer = AdamW(model.parameters(), lr=3e-5)\nnum_training_steps = len(train_dataloader) * 3  # 3 epochs\nlr_scheduler = get_scheduler(\n    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n)\n\n# Metric\nmetric = load_metric('sacrebleu')\n\n# Prepare model, optimizer, and dataloaders with `accelerate`\nmodel, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n    model, optimizer, train_dataloader, eval_dataloader\n)\n\n# Variables to store training and validation results\ntraining_losses = []\nvalidation_bleu_scores = []\n\n# Custom training loop with max_length set to 32\nprint(\"Starting training...\")\nnum_epochs = 3\ntrain_start = time.time()\nfor epoch in range(num_epochs):\n    model.train()\n    epoch_loss = 0.0\n    epoch_start = time.time()\n    for step, batch in enumerate(train_dataloader):\n        with accelerator.accumulate(model):\n            outputs = model(**batch)\n            loss = outputs.loss\n            epoch_loss += loss.item()\n\n            # Backward pass\n            accelerator.backward(loss)\n            optimizer.step()\n            lr_scheduler.step()\n            optimizer.zero_grad()\n\n        # Logging every 300 steps\n        if step % 300 == 0 and step != 0:\n            avg_loss = epoch_loss / (step + 1)\n            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{step}/{len(train_dataloader)}], Loss: {avg_loss:.4f}\")\n\n    training_losses.append(epoch_loss / len(train_dataloader))\n    print(f\"Epoch [{epoch+1}/{num_epochs}] completed in {time.time() - epoch_start:.2f} seconds\")\n\n    # Validation\n    validation_start = time.time()\n    model.eval()\n    all_preds = []\n    all_labels = []\n    for batch in eval_dataloader:\n        with torch.no_grad():\n            outputs = accelerator.unwrap_model(model).generate(\n                input_ids=batch[\"input_ids\"],\n                attention_mask=batch[\"attention_mask\"],\n                max_length=32,\n                num_beams=4\n            )\n        labels = batch[\"labels\"]\n        labels = np.where(labels.cpu().numpy() != -100, labels.cpu().numpy(), tokenizer.pad_token_id)\n        decoded_preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n        all_preds.extend(decoded_preds)\n        all_labels.extend(decoded_labels)\n    result = metric.compute(predictions=all_preds, references=[[label] for label in all_labels])\n    validation_bleu_scores.append(result['score'])\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Validation BLEU Score: {result['score']:.2f}\")\n    print(f\"Validation completed in {time.time() - validation_start:.2f} seconds\")\n\nprint(f\"Training completed in {time.time() - train_start:.2f} seconds\")\n\n# Plot training loss and validation BLEU score\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.plot(range(1, num_epochs + 1), training_losses, marker='o', label=\"Training Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training Loss over Epochs\")\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(range(1, num_epochs + 1), validation_bleu_scores, marker='o', color='orange', label=\"Validation BLEU Score\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"BLEU Score\")\nplt.title(\"Validation BLEU Score over Epochs\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n# Save model in Hugging Face format\nmodel_dir = \"./llama3_1B_eng_hing\"\naccelerator.wait_for_everyone()\nunwrapped_model = accelerator.unwrap_model(model)\nunwrapped_model.save_pretrained(model_dir, save_function=accelerator.save)\ntokenizer.save_pretrained(model_dir)\n\n# Save model as .pt file\ntorch.save(unwrapped_model.state_dict(), \"llama3_1B_eng_hing.pt\")\nprint(\"Model saved as 'llama3_1B_eng_hing.pt'\")\n\n# Test Evaluation and BLEU Score Distribution\nprint(\"Evaluating on the test set...\")\ntest_start = time.time()\ntest_dataloader = DataLoader(test_dataset, collate_fn=data_collator, batch_size=32)\nmodel.eval()\nall_preds = []\nall_labels = []\nindividual_bleu_scores = []\nfor batch in test_dataloader:\n    with torch.no_grad():\n        outputs = accelerator.unwrap_model(model).generate(\n            input_ids=batch[\"input_ids\"],\n            attention_mask=batch[\"attention_mask\"],\n            max_length=32,\n            num_beams=4\n        )\n    labels = batch[\"labels\"]\n    labels = np.where(labels.cpu().numpy() != -100, labels.cpu().numpy(), tokenizer.pad_token_id)\n    decoded_preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    all_preds.extend(decoded_preds)\n    all_labels.extend(decoded_labels)\n\n    # Calculate individual BLEU scores for each prediction\n    for pred, label in zip(decoded_preds, decoded_labels):\n        bleu = metric.compute(predictions=[pred], references=[[label]])\n        individual_bleu_scores.append(bleu[\"score\"])\n\nprint(f\"Test evaluation completed in {time.time() - test_start:.2f} seconds\")\n\n# Overall BLEU score on the test set\ntest_result = metric.compute(predictions=all_preds, references=[[label] for label in all_labels])\nprint(f\"Test BLEU Score: {test_result['score']:.2f}\")\n\n# Plot BLEU score distribution with density curve and mean line\nplt.figure(figsize=(10, 6))\nsns.histplot(individual_bleu_scores, bins=20, kde=True, color='skyblue', edgecolor='black', alpha=0.7)\nmean_bleu = np.mean(individual_bleu_scores)\nplt.axvline(mean_bleu, color='red', linestyle='--', label=f'Mean BLEU Score: {mean_bleu:.2f}')\nplt.title(\"BLEU Score Distribution on Test Set\")\nplt.xlabel(\"BLEU Score\")\nplt.ylabel(\"Frequency\")\nplt.legend()\nplt.show()\n\n# Display sample translations from test set\ndef display_sample_translations(num_samples=5):\n    sampled_indices = random.sample(range(len(test_dataset)), num_samples)\n    for idx in sampled_indices:\n        input_text = test_dataset[idx]['translation']['en']\n        reference_text = test_dataset[idx]['translation']['hi_ng']\n        input_ids = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=32).input_ids.to(device)\n        with torch.no_grad():\n            generated_tokens = accelerator.unwrap_model(model).generate(\n                input_ids, max_length=32, num_beams=4\n            )\n        predicted_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n\n        print(\"\\nSample Translation:\")\n        print(f\"Input (English): {input_text}\")\n        print(f\"Reference (Hinglish): {reference_text}\")\n        print(f\"Prediction (Model Output): {predicted_text}\")\n\n# Display 5 sample translations from the test set\ndisplay_sample_translations(num_samples=5)\n\n# Test with a custom sentence\ndef translate_sentence(sentence):\n    input_text = sentence  # Removed \"translate English to Hinglish:\" prefix for consistency\n    input_ids = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=32).input_ids.to(device)\n    with torch.no_grad():\n        generated_tokens = accelerator.unwrap_model(model).generate(\n            input_ids, max_length=32, num_beams=4\n        )\n    return tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n\ncustom_sentence = \"Where is my bag?\"\ntranslated_sentence = translate_sentence(custom_sentence)\nprint(\"\\nCustom Translation:\")\nprint(f\"Input: {custom_sentence}\")\nprint(f\"Translated Output: {translated_sentence}\")\n\n# Total execution time\nprint(f\"Total execution time: {time.time() - start_time:.2f} seconds\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T21:05:19.431602Z","iopub.execute_input":"2024-11-12T21:05:19.431996Z","iopub.status.idle":"2024-11-12T21:05:53.278602Z","shell.execute_reply.started":"2024-11-12T21:05:19.431957Z","shell.execute_reply":"2024-11-12T21:05:53.274000Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Using device: cuda\nTraining set size: 45384\nValidation set size: 5673\nTest set size: 5673\nDataset loaded in 2.01 seconds\n","output_type":"stream"},{"name":"stderr","text":"The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \nThe class this function is called from is 'LlamaTokenizerFast'.\nUnused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 3,932,160 || all params: 1,239,746,560 || trainable%: 0.3172\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/45384 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c12ed929a26242fcabdc285d7af47915"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5673 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d545c8373d5e4b6c90dfbbd9240e6b71"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5673 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df9fbcbe58c042e89040cb6c55e98c6e"}},"metadata":{}},{"name":"stdout","text":"Data preprocessed in 12.79 seconds\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Starting training...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[43], line 176\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m accelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m--> 176\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m         loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m    178\u001b[0m         epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/peft_model.py:1644\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1642\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1643\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1644\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1645\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1646\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1647\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1648\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1649\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1650\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1651\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1652\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1653\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1655\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1656\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1657\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:197\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1214\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m   1212\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1214\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_function\u001b[49m(logits\u001b[38;5;241m=\u001b[39mlogits, labels\u001b[38;5;241m=\u001b[39mlabels, vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mvocab_size, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs)\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m   1217\u001b[0m     output \u001b[38;5;241m=\u001b[39m (logits,) \u001b[38;5;241m+\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m:]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1729\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1727\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1728\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1729\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mAttributeError\u001b[0m: 'LlamaForCausalLM' object has no attribute 'loss_function'"],"ename":"AttributeError","evalue":"'LlamaForCausalLM' object has no attribute 'loss_function'","output_type":"error"}],"execution_count":43},{"cell_type":"code","source":"# Replace 'your_actual_hugging_face_token_here' with your actual token\nwith open('.env', 'w') as f:\n    f.write('HF_TOKEN=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\\n')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T14:42:00.053194Z","iopub.execute_input":"2024-11-12T14:42:00.053608Z","iopub.status.idle":"2024-11-12T14:42:00.058986Z","shell.execute_reply.started":"2024-11-12T14:42:00.053545Z","shell.execute_reply":"2024-11-12T14:42:00.057917Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"ls -la\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T14:42:16.122232Z","iopub.execute_input":"2024-11-12T14:42:16.122695Z","iopub.status.idle":"2024-11-12T14:42:17.191841Z","shell.execute_reply.started":"2024-11-12T14:42:16.122652Z","shell.execute_reply":"2024-11-12T14:42:17.190693Z"}},"outputs":[{"name":"stdout","text":"total 24\ndrwxr-xr-x 3 root root 4096 Nov 12 14:42  \u001b[0m\u001b[01;34m.\u001b[0m/\ndrwxr-xr-x 5 root root 4096 Nov 12 13:47  \u001b[01;34m..\u001b[0m/\n-rw-r--r-- 1 root root   47 Nov 12 14:42  .env\ndrwxr-xr-x 2 root root 4096 Nov 12 13:47  \u001b[01;34m.virtual_documents\u001b[0m/\n-rw-r--r-- 1 root root 6847 Nov 12 14:25 '=0.39.0'\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# Install necessary packages\n# Ensure you run this cell in an environment where you have permission to install packages.\n!pip install --upgrade transformers datasets sacrebleu sentencepiece evaluate peft matplotlib seaborn accelerate bitsandbytes>=0.39.0 python-dotenv\n\n# Import necessary libraries\nimport os\nimport torch\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time  # For tracking time\n\nfrom dotenv import load_dotenv  # For loading environment variables securely\nfrom datasets import load_dataset\nfrom transformers import (\n    LlamaForCausalLM,\n    PreTrainedTokenizerFast,  # Use the base fast tokenizer\n    DataCollatorForSeq2Seq,\n    get_scheduler\n)\nfrom torch.optim import AdamW  # Use PyTorch's AdamW instead of Transformers'\nfrom peft import get_peft_model, prepare_model_for_kbit_training, LoraConfig, TaskType\nfrom evaluate import load as load_metric\nfrom torch.utils.data import DataLoader\nfrom accelerate import Accelerator\n\n# ----------------------------\n# 🛡️ **Securely Load Your Hugging Face Token**\n# ----------------------------\n\n# Load environment variables from a .env file if present\nload_dotenv()\n\n# Retrieve the Hugging Face token from environment variables\nhf_token = os.getenv(\"HF_TOKEN\")\nif hf_token is None:\n    raise ValueError(\"Hugging Face token not found. Please set it as an environment variable 'HF_TOKEN'.\")\n\n# ----------------------------\n# 🕒 **Initialize and Configure the Training Environment**\n# ----------------------------\n\n# Start timer for total execution time\nstart_time = time.time()\n\n# Set environment variable to disable parallelism in tokenizers to prevent potential deadlocks\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n# Initialize accelerator for optimized training across multiple GPUs/TPUs\naccelerator = Accelerator()\n\n# Check if GPUs are available and set device\ndevice = accelerator.device\nprint(f\"Using device: {device}\")\n\n# Set seed for reproducibility\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(seed)\n\n# ----------------------------\n# 📚 **Load and Prepare the Dataset**\n# ----------------------------\n\n# Track time for dataset loading\nload_dataset_start = time.time()\n\n# Load the English-to-Hinglish translation dataset\ndataset = load_dataset('findnitai/english-to-hinglish')\n\n# Reduce the dataset size to 30% for faster experimentation\nsample_fraction = 0.3\nfull_train_size = len(dataset['train'])\nselected_size = int(full_train_size * sample_fraction)\ndataset = dataset['train'].shuffle(seed=seed).select(range(selected_size))\n\n# Split the dataset into train, validation, and test sets\ntrain_test_split = dataset.train_test_split(test_size=0.2, seed=seed)\ntest_valid_split = train_test_split['test'].train_test_split(test_size=0.5, seed=seed)\n\ntrain_dataset = train_test_split['train']\neval_dataset = test_valid_split['test']\ntest_dataset = test_valid_split['train']\n\n# Print dataset sizes\nprint(f\"Training set size: {len(train_dataset)}\")\nprint(f\"Validation set size: {len(eval_dataset)}\")\nprint(f\"Test set size: {len(test_dataset)}\")\n\n# Dataset loading time\nprint(f\"Dataset loaded in {time.time() - load_dataset_start:.2f} seconds\")\n\n# ----------------------------\n# 🔑 **Initialize Tokenizer and Model**\n# ----------------------------\n\n# Initialize tokenizer with authentication\nmodel_name = \"unsloth/Llama-3.2-1B-bnb-4bit\"\n\n# Use PreTrainedTokenizerFast for compatibility\ntokenizer = PreTrainedTokenizerFast.from_pretrained(model_name, token=hf_token, legacy=False)\n\n# Define padding token if not already defined\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n    print(\"Pad token not found. Setting pad_token to eos_token.\")\n\n# Optional: Update model config with pad_token_id\n# For many models, setting pad_token in the tokenizer is sufficient\npad_token_id = tokenizer.pad_token_id\n\n# Load the model with authentication\nmodel = LlamaForCausalLM.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,  # Ensure consistent dtype\n    token=hf_token                # Replaced use_auth_token with token\n)\n\n# Prepare model for k-bit (4-bit) training using PEFT\nmodel = prepare_model_for_kbit_training(model)\n\n# Enable gradient checkpointing for memory optimization (optional)\nmodel.gradient_checkpointing_enable()\n\n# ----------------------------\n# 🛠️ **Configure and Apply LoRA**\n# ----------------------------\n\n# Define LoRA configuration\nlora_config = LoraConfig(\n    task_type=TaskType.CAUSAL_LM,  # Correct task type for causal language models\n    inference_mode=False,\n    r=8,\n    lora_alpha=32,\n    lora_dropout=0.1,\n    target_modules=[\"gate_proj\", \"up_proj\", \"down_proj\"]  # Target specific supported submodules\n)\n\n# Apply LoRA to the model\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()  # Optional: Print trainable parameters\n\n# ----------------------------\n# 🗣️ **Preprocess the Dataset**\n# ----------------------------\n\n# Define preprocessing function for English-to-Hinglish translation\ndef preprocess_function(examples):\n    inputs = [ex['en'] for ex in examples['translation']]\n    targets = [ex['hi_ng'] for ex in examples['translation']]\n    model_inputs = tokenizer(inputs, max_length=32, padding=\"max_length\", truncation=True)\n    labels = tokenizer(targets, max_length=32, padding=\"max_length\", truncation=True)\n    # Replace padding token id's of the labels by -100 to ignore them in the loss\n    labels[\"input_ids\"] = [\n        [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n    ]\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\n# Track preprocessing time\npreprocess_start = time.time()\n\n# Apply preprocessing to the datasets\ntrain_dataset = train_dataset.map(preprocess_function, batched=True, remove_columns=[\"translation\"], num_proc=4)\neval_dataset = eval_dataset.map(preprocess_function, batched=True, remove_columns=[\"translation\"], num_proc=4)\ntest_dataset = test_dataset.map(preprocess_function, batched=True, remove_columns=[\"translation\"], num_proc=4)\n\nprint(f\"Data preprocessed in {time.time() - preprocess_start:.2f} seconds\")\n\n# ----------------------------\n# 🛠️ **Set Up Data Collator and Dataloaders**\n# ----------------------------\n\n# Initialize Data Collator for Seq2Seq tasks\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding=True)\n\n# Define DataLoaders\ntrain_dataloader = DataLoader(train_dataset, shuffle=True, collate_fn=data_collator, batch_size=32)\neval_dataloader = DataLoader(eval_dataset, collate_fn=data_collator, batch_size=32)\ntest_dataloader = DataLoader(test_dataset, collate_fn=data_collator, batch_size=32)\n\n# ----------------------------\n# 🛠️ **Initialize Optimizer and Scheduler**\n# ----------------------------\n\n# Initialize PyTorch's AdamW optimizer\noptimizer = AdamW(model.parameters(), lr=3e-5)\n\n# Define the number of training steps\nnum_epochs = 3\nnum_training_steps = len(train_dataloader) * num_epochs\n\n# Initialize a linear scheduler with no warmup steps\nlr_scheduler = get_scheduler(\n    name=\"linear\",\n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=num_training_steps\n)\n\n# ----------------------------\n# 🧮 **Initialize Evaluation Metrics**\n# ----------------------------\n\n# Load the SacreBLEU metric for evaluation\nmetric = load_metric('sacrebleu')\n\n# ----------------------------\n# 🛠️ **Prepare Model and Data with Accelerator**\n# ----------------------------\n\n# Prepare the model, optimizer, and dataloaders with accelerator\nmodel, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n    model, optimizer, train_dataloader, eval_dataloader\n)\n\n# ----------------------------\n# 🏋️‍♂️ **Training Loop**\n# ----------------------------\n\n# Variables to store training and validation results\ntraining_losses = []\nvalidation_bleu_scores = []\n\nprint(\"Starting training...\")\ntrain_start = time.time()\n\nfor epoch in range(num_epochs):\n    model.train()\n    epoch_loss = 0.0\n    epoch_start = time.time()\n    \n    for step, batch in enumerate(train_dataloader):\n        with accelerator.accumulate(model):\n            outputs = model(**batch)\n            loss = outputs.loss\n            epoch_loss += loss.item()\n\n            # Backward pass\n            accelerator.backward(loss)\n            optimizer.step()\n            lr_scheduler.step()\n            optimizer.zero_grad()\n\n        # Logging every 300 steps\n        if step % 300 == 0 and step != 0:\n            avg_loss = epoch_loss / (step + 1)\n            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{step}/{len(train_dataloader)}], Loss: {avg_loss:.4f}\")\n\n    # Record average loss for the epoch\n    average_epoch_loss = epoch_loss / len(train_dataloader)\n    training_losses.append(average_epoch_loss)\n    print(f\"Epoch [{epoch+1}/{num_epochs}] completed in {time.time() - epoch_start:.2f} seconds\")\n\n    # Validation Phase\n    validation_start = time.time()\n    model.eval()\n    all_preds = []\n    all_labels = []\n    \n    for batch in eval_dataloader:\n        with torch.no_grad():\n            outputs = accelerator.unwrap_model(model).generate(\n                input_ids=batch[\"input_ids\"],\n                attention_mask=batch[\"attention_mask\"],\n                max_length=32,\n                num_beams=4\n            )\n        labels = batch[\"labels\"]\n        # Replace -100 with pad_token_id for decoding\n        labels = np.where(labels.cpu().numpy() != -100, labels.cpu().numpy(), tokenizer.pad_token_id)\n        decoded_preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n        all_preds.extend(decoded_preds)\n        all_labels.extend(decoded_labels)\n    \n    # Compute BLEU score\n    result = metric.compute(predictions=all_preds, references=[[label] for label in all_labels])\n    validation_bleu_scores.append(result['score'])\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Validation BLEU Score: {result['score']:.2f}\")\n    print(f\"Validation completed in {time.time() - validation_start:.2f} seconds\")\n\nprint(f\"Training completed in {time.time() - train_start:.2f} seconds\")\n\n# ----------------------------\n# 📈 **Plot Training Loss and Validation BLEU Scores**\n# ----------------------------\n\nplt.figure(figsize=(12, 6))\n\n# Plot Training Loss\nplt.subplot(1, 2, 1)\nplt.plot(range(1, num_epochs + 1), training_losses, marker='o', label=\"Training Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training Loss over Epochs\")\nplt.legend()\n\n# Plot Validation BLEU Score\nplt.subplot(1, 2, 2)\nplt.plot(range(1, num_epochs + 1), validation_bleu_scores, marker='o', color='orange', label=\"Validation BLEU Score\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"BLEU Score\")\nplt.title(\"Validation BLEU Score over Epochs\")\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n# ----------------------------\n# 💾 **Save the Fine-Tuned Model**\n# ----------------------------\n\n# Define the directory to save the model\nmodel_dir = \"./llama3_1B_eng_hing\"\n\n# Ensure all processes are synchronized\naccelerator.wait_for_everyone()\n\n# Unwrap the model to access the underlying model for saving\nunwrapped_model = accelerator.unwrap_model(model)\n\n# Save the model in Hugging Face format\nunwrapped_model.save_pretrained(model_dir, save_function=accelerator.save)\ntokenizer.save_pretrained(model_dir)\n\n# Save the model's state dictionary as a .pt file\ntorch.save(unwrapped_model.state_dict(), \"llama3_1B_eng_hing.pt\")\nprint(\"Model saved as 'llama3_1B_eng_hing.pt'\")\n\n# ----------------------------\n# 🧪 **Evaluate on the Test Set and Analyze BLEU Scores**\n# ----------------------------\n\nprint(\"Evaluating on the test set...\")\ntest_start = time.time()\nmodel.eval()\nall_test_preds = []\nall_test_labels = []\nindividual_bleu_scores = []\n\nfor batch in test_dataloader:\n    with torch.no_grad():\n        outputs = accelerator.unwrap_model(model).generate(\n            input_ids=batch[\"input_ids\"],\n            attention_mask=batch[\"attention_mask\"],\n            max_length=32,\n            num_beams=4\n        )\n    labels = batch[\"labels\"]\n    # Replace -100 with pad_token_id for decoding\n    labels = np.where(labels.cpu().numpy() != -100, labels.cpu().numpy(), tokenizer.pad_token_id)\n    decoded_preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    all_test_preds.extend(decoded_preds)\n    all_test_labels.extend(decoded_labels)\n\n    # Calculate individual BLEU scores for each prediction\n    for pred, label in zip(decoded_preds, decoded_labels):\n        bleu = metric.compute(predictions=[pred], references=[[label]])\n        individual_bleu_scores.append(bleu[\"score\"])\n\nprint(f\"Test evaluation completed in {time.time() - test_start:.2f} seconds\")\n\n# Compute overall BLEU score on the test set\ntest_result = metric.compute(predictions=all_test_preds, references=[[label] for label in all_test_labels])\nprint(f\"Test BLEU Score: {test_result['score']:.2f}\")\n\n# ----------------------------\n# 📊 **Plot BLEU Score Distribution**\n# ----------------------------\n\nplt.figure(figsize=(10, 6))\nsns.histplot(individual_bleu_scores, bins=20, kde=True, color='skyblue', edgecolor='black', alpha=0.7)\nmean_bleu = np.mean(individual_bleu_scores)\nplt.axvline(mean_bleu, color='red', linestyle='--', label=f'Mean BLEU Score: {mean_bleu:.2f}')\nplt.title(\"BLEU Score Distribution on Test Set\")\nplt.xlabel(\"BLEU Score\")\nplt.ylabel(\"Frequency\")\nplt.legend()\nplt.show()\n\n# ----------------------------\n# 📝 **Display Sample Translations from the Test Set**\n# ----------------------------\n\ndef display_sample_translations(num_samples=5):\n    sampled_indices = random.sample(range(len(test_dataset)), num_samples)\n    for idx in sampled_indices:\n        input_text = test_dataset[idx]['translation']['en']\n        reference_text = test_dataset[idx]['translation']['hi_ng']\n        input_ids = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=32).input_ids.to(device)\n        with torch.no_grad():\n            generated_tokens = accelerator.unwrap_model(model).generate(\n                input_ids, max_length=32, num_beams=4\n            )\n        predicted_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n\n        print(\"\\n📝 **Sample Translation:**\")\n        print(f\"**Input (English):** {input_text}\")\n        print(f\"**Reference (Hinglish):** {reference_text}\")\n        print(f\"**Prediction (Model Output):** {predicted_text}\")\n\n# Display 5 sample translations from the test set\ndisplay_sample_translations(num_samples=5)\n\n# ----------------------------\n# 🧪 **Test with a Custom Sentence**\n# ----------------------------\n\ndef translate_sentence(sentence):\n    input_text = sentence  # Removed \"translate English to Hinglish:\" prefix for consistency\n    input_ids = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=32).input_ids.to(device)\n    with torch.no_grad():\n        generated_tokens = accelerator.unwrap_model(model).generate(\n            input_ids, max_length=32, num_beams=4\n        )\n    return tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n\ncustom_sentence = \"Where is my bag?\"\ntranslated_sentence = translate_sentence(custom_sentence)\nprint(\"\\n🧪 **Custom Translation:**\")\nprint(f\"**Input:** {custom_sentence}\")\nprint(f\"**Translated Output:** {translated_sentence}\")\n\n# ----------------------------\n# ⏰ **Report Total Execution Time**\n# ----------------------------\n\nprint(f\"\\n⏱️ **Total Execution Time:** {time.time() - start_time:.2f} seconds\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T21:04:00.511912Z","iopub.execute_input":"2024-11-12T21:04:00.512449Z","iopub.status.idle":"2024-11-12T21:04:23.297444Z","shell.execute_reply.started":"2024-11-12T21:04:00.512395Z","shell.execute_reply":"2024-11-12T21:04:23.295856Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Using device: cuda\nTraining set size: 45384\nValidation set size: 5673\nTest set size: 5673\nDataset loaded in 2.90 seconds\n","output_type":"stream"},{"name":"stderr","text":"Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 3,932,160 || all params: 1,239,746,560 || trainable%: 0.3172\nData preprocessed in 0.72 seconds\nStarting training...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[42], line 240\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m accelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m--> 240\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m         loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m    242\u001b[0m         epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/peft_model.py:1644\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1642\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1643\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1644\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1645\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1646\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1647\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1648\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1649\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1650\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1651\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1652\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1653\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1655\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1656\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1657\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:197\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1214\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m   1212\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1214\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_function\u001b[49m(logits\u001b[38;5;241m=\u001b[39mlogits, labels\u001b[38;5;241m=\u001b[39mlabels, vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mvocab_size, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs)\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m   1217\u001b[0m     output \u001b[38;5;241m=\u001b[39m (logits,) \u001b[38;5;241m+\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m:]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1729\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1727\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1728\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1729\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mAttributeError\u001b[0m: 'LlamaForCausalLM' object has no attribute 'loss_function'"],"ename":"AttributeError","evalue":"'LlamaForCausalLM' object has no attribute 'loss_function'","output_type":"error"}],"execution_count":42},{"cell_type":"code","source":"# ----------------------------\n# 📦 **1. Install Necessary Packages**\n# ----------------------------\n\n# Ensure you run this cell in an environment where you have permission to install packages.\n!pip install --upgrade transformers datasets sacrebleu sentencepiece evaluate peft matplotlib seaborn accelerate bitsandbytes>=0.39.0 python-dotenv\n\n# ----------------------------\n# 📚 **2. Import Necessary Libraries**\n# ----------------------------\n\nimport os\nimport torch\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time  # For tracking time\n\nfrom dotenv import load_dotenv  # For loading environment variables securely\nfrom datasets import load_dataset\nfrom transformers import (\n    LlamaForCausalLM,\n    PreTrainedTokenizerFast,\n    DataCollatorForSeq2Seq,\n    get_scheduler\n)\nfrom torch.optim import AdamW\nfrom peft import get_peft_model, prepare_model_for_kbit_training, LoraConfig, TaskType\nfrom evaluate import load as load_metric\nfrom torch.utils.data import DataLoader\nfrom accelerate import Accelerator\n\n# ----------------------------\n# 🛡️ **3. Securely Load Your Hugging Face Token**\n# ----------------------------\n\n# with open('.env', 'w') as f:\n#     f.write('HF_TOKEN=your_actual_hugging_face_token_here')  # Replace with your token\n\nload_dotenv()\nhf_token = os.getenv(\"HF_TOKEN\")\nif hf_token is None:\n    raise ValueError(\"Hugging Face token not found. Please set it correctly in the `.env` file.\")\n\n# ----------------------------\n# 🕒 **4. Initialize and Configure the Training Environment**\n# ----------------------------\n\nstart_time = time.time()\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\naccelerator = Accelerator()\ndevice = accelerator.device\nprint(f\"Using device: {device}\")\n\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(seed)\n\n# ----------------------------\n# 📚 **5. Load and Prepare the Dataset**\n# ----------------------------\n\nload_dataset_start = time.time()\ndataset = load_dataset('findnitai/english-to-hinglish')\nsample_fraction = 0.3\ndataset = dataset['train'].shuffle(seed=seed).select(range(int(len(dataset['train']) * sample_fraction)))\ntrain_test_split = dataset.train_test_split(test_size=0.2, seed=seed)\ntest_valid_split = train_test_split['test'].train_test_split(test_size=0.5, seed=seed)\n\ntrain_dataset = train_test_split['train']\neval_dataset = test_valid_split['test']\ntest_dataset = test_valid_split['train']\nprint(f\"Training set size: {len(train_dataset)}, Validation set size: {len(eval_dataset)}, Test set size: {len(test_dataset)}\")\n\n# ----------------------------\n# 🔑 **6. Initialize Tokenizer and Model**\n# ----------------------------\n\nmodel_name = \"unsloth/Llama-3.2-1B-bnb-4bit\"\ntokenizer = PreTrainedTokenizerFast.from_pretrained(model_name, token=hf_token, legacy=False)\n\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\npad_token_id = tokenizer.pad_token_id\n\nmodel = LlamaForCausalLM.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    use_auth_token=hf_token\n)\n\nmodel = prepare_model_for_kbit_training(model)\nmodel.gradient_checkpointing_enable()\n\n# ----------------------------\n# 🛠️ **7. Configure and Apply LoRA**\n# ----------------------------\n\nlora_config = LoraConfig(\n    task_type=TaskType.CAUSAL_LM,\n    inference_mode=False,\n    r=8,\n    lora_alpha=32,\n    lora_dropout=0.1,\n    target_modules=[\"gate_proj\", \"up_proj\", \"down_proj\"]\n)\n\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()\n\n# ----------------------------\n# 🗣️ **8. Preprocess the Dataset**\n# ----------------------------\n\ndef preprocess_function(examples):\n    inputs = [ex['en'] for ex in examples['translation']]\n    targets = [ex['hi_ng'] for ex in examples['translation']]\n    model_inputs = tokenizer(inputs, max_length=32, padding=\"max_length\", truncation=True)\n    labels = tokenizer(targets, max_length=32, padding=\"max_length\", truncation=True)\n    labels[\"input_ids\"] = [\n        [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n    ]\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\ntrain_dataset = train_dataset.map(preprocess_function, batched=True, remove_columns=[\"translation\"], num_proc=4)\neval_dataset = eval_dataset.map(preprocess_function, batched=True, remove_columns=[\"translation\"], num_proc=4)\ntest_dataset = test_dataset.map(preprocess_function, batched=True, remove_columns=[\"translation\"], num_proc=4)\n\n# ----------------------------\n# 🛠️ **9. Set Up Data Collator and Dataloaders**\n# ----------------------------\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding=True)\ntrain_dataloader = DataLoader(train_dataset, shuffle=True, collate_fn=data_collator, batch_size=64, num_workers=4, pin_memory=True)\neval_dataloader = DataLoader(eval_dataset, collate_fn=data_collator, batch_size=64, num_workers=4, pin_memory=True)\ntest_dataloader = DataLoader(test_dataset, collate_fn=data_collator, batch_size=64, num_workers=4, pin_memory=True)\n\n# ----------------------------\n# 🛠️ **10. Initialize Optimizer and Scheduler**\n# ----------------------------\n\noptimizer = AdamW(model.parameters(), lr=3e-5)\nnum_epochs = 2\nnum_training_steps = len(train_dataloader) * num_epochs\nlr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n\n# ----------------------------\n# 🧮 **11. Initialize Evaluation Metrics**\n# ----------------------------\n\nmetric = load_metric('sacrebleu')\n\n# ----------------------------\n# 🛠️ **12. Prepare Model and Data with Accelerator**\n# ----------------------------\n\nmodel, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(model, optimizer, train_dataloader, eval_dataloader)\n\n# ----------------------------\n# 🏋️‍♂️ **13. Training Loop**\n# ----------------------------\n\ntraining_losses = []\nvalidation_bleu_scores = []\n\nprint(\"Starting training...\")\nfor epoch in range(num_epochs):\n    model.train()\n    epoch_loss = 0.0\n    \n    for step, batch in enumerate(train_dataloader):\n        with accelerator.accumulate(model):\n            outputs = model(**batch)\n            loss = outputs.loss\n            epoch_loss += loss.item()\n\n            accelerator.backward(loss)\n            optimizer.step()\n            lr_scheduler.step()\n            optimizer.zero_grad()\n\n        if step % 300 == 0 and step != 0:\n            avg_loss = epoch_loss / (step + 1)\n            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{step}/{len(train_dataloader)}], Loss: {avg_loss:.4f}\")\n\n    training_losses.append(epoch_loss / len(train_dataloader))\n\n    model.eval()\n    all_preds = []\n    all_labels = []\n    \n    for batch in eval_dataloader:\n        with torch.no_grad():\n            outputs = accelerator.unwrap_model(model).generate(\n                input_ids=batch[\"input_ids\"],\n                attention_mask=batch[\"attention_mask\"],\n                max_length=32,\n                num_beams=4\n            )\n        labels = batch[\"labels\"]\n        labels = np.where(labels.cpu().numpy() != -100, labels.cpu().numpy(), tokenizer.pad_token_id)\n        decoded_preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n        all_preds.extend(decoded_preds)\n        all_labels.extend(decoded_labels)\n    \n    result = metric.compute(predictions=all_preds, references=[[label] for label in all_labels])\n    validation_bleu_scores.append(result['score'])\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Validation BLEU Score: {result['score']:.2f}\")\n\n# ----------------------------\n# 📈 **14. Plot Training Loss and Validation BLEU Scores**\n# ----------------------------\n\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.plot(range(1, num_epochs + 1), training_losses, marker='o', label=\"Training Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training Loss over Epochs\")\nplt.legend()\nplt.subplot(1, 2, 2)\nplt.plot(range(1, num_epochs + 1), validation_bleu_scores, marker='o', color='orange', label=\"Validation BLEU Score\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"BLEU Score\")\nplt.title(\"Validation BLEU Score over Epochs\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n# ----------------------------\n# 💾 **15. Save the Fine-Tuned Model**\n# ----------------------------\n\nmodel_dir = \"./llama3_1B_eng_hing\"\naccelerator.wait_for_everyone()\nunwrapped_model = accelerator.unwrap_model(model)\nunwrapped_model.save_pretrained(model_dir, save_function=accelerator.save)\ntokenizer.save_pretrained(model_dir)\ntorch.save(unwrapped_model.state_dict(), \"llama3_1B_eng_hing.pt\")\nprint(\"Model saved as 'llama3_1B_eng_hing.pt'\")\n\n# ----------------------------\n# ⏰ **20. Report Total Execution Time**\n# ----------------------------\n\nprint(f\"\\n⏱️ **Total Execution Time:** {time.time() - start_time:.2f} seconds\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T21:02:15.095475Z","iopub.execute_input":"2024-11-12T21:02:15.095963Z","iopub.status.idle":"2024-11-12T21:02:45.729366Z","shell.execute_reply.started":"2024-11-12T21:02:15.095900Z","shell.execute_reply":"2024-11-12T21:02:45.725305Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Using device: cuda\nTraining set size: 45384, Validation set size: 5673, Test set size: 5673\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:3274: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\nUnused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 3,932,160 || all params: 1,239,746,560 || trainable%: 0.3172\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/45384 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73a171ee60c444d3ae1d5fb33fd1394b"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/5673 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4217461610944c94b16a8a37744ce9c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/5673 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92a954fc9d404a38abab48153a7bfb79"}},"metadata":{}},{"name":"stdout","text":"Starting training...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[41], line 179\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m accelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m--> 179\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m         loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m    181\u001b[0m         epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/peft_model.py:1644\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1642\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1643\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1644\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1645\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1646\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1647\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1648\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1649\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1650\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1651\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1652\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1653\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1655\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1656\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1657\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:197\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1214\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m   1212\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1214\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_function\u001b[49m(logits\u001b[38;5;241m=\u001b[39mlogits, labels\u001b[38;5;241m=\u001b[39mlabels, vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mvocab_size, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs)\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m   1217\u001b[0m     output \u001b[38;5;241m=\u001b[39m (logits,) \u001b[38;5;241m+\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m:]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1729\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1727\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1728\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1729\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mAttributeError\u001b[0m: 'LlamaForCausalLM' object has no attribute 'loss_function'"],"ename":"AttributeError","evalue":"'LlamaForCausalLM' object has no attribute 'loss_function'","output_type":"error"}],"execution_count":41},{"cell_type":"markdown","source":"# Helsinki-NLP/opus-mt-en-ROMANCE","metadata":{}},{"cell_type":"code","source":"# Install necessary packages\n!pip install transformers datasets sacrebleu sentencepiece evaluate\n\n# Import necessary libraries\nimport os\nimport torch\nimport datasets\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time\nfrom datasets import load_dataset\nfrom evaluate import load as load_metric\nfrom transformers import (\n    AutoConfig,\n    AutoTokenizer,\n    AutoModelForSeq2SeqLM,\n    DataCollatorForSeq2Seq,\n)\nfrom torch.utils.data import DataLoader\nfrom torch.optim import AdamW\nfrom transformers import get_linear_schedule_with_warmup\nimport random\n\n# Set environment variable to disable parallelism in tokenizers\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n# Check if GPUs are available and set device\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    n_gpu = torch.cuda.device_count()\n    print(f\"Using device: {device}\")\n    print(f\"Number of GPUs available: {n_gpu}\")\nelse:\n    device = torch.device(\"cpu\")\n    n_gpu = 0\n    print(\"No GPU available, using CPU.\")\n\n# Set seed for reproducibility\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif n_gpu > 0:\n    torch.cuda.manual_seed_all(seed)\n\n# Timing: Start\nstart_time = time.time()\n\n# Initialize model, tokenizer, and config\nmodel_name = \"Helsinki-NLP/opus-mt-en-ROMANCE\"\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\nconfig = AutoConfig.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name, config=config)\n\n# Move model to device before wrapping with DataParallel\nmodel.to(device)\n\n# Wrap the model with DataParallel if multiple GPUs are available\nif n_gpu > 1:\n    model = torch.nn.DataParallel(model)\n    print(\"Using DataParallel for multi-GPU training.\")\n\n# Load the dataset\ndataset_load_start = time.time()\ndataset = load_dataset('findnitai/english-to-hinglish')\ndataset = dataset['train']\ndataset = datasets.DatasetDict({'train': dataset})\nraw_datasets = dataset['train'].train_test_split(test_size=0.1, seed=seed)\ntrain_dataset = raw_datasets['train']\neval_dataset = raw_datasets['test']\nprint(f\"Dataset loaded in {time.time() - dataset_load_start:.2f} seconds\")\n\n# Print dataset sizes\nprint(f\"Training samples: {len(train_dataset)}, Evaluation samples: {len(eval_dataset)}\")\n\n# Preprocessing parameters\nsource_prefix = \"translate English to Hinglish: \"\nsource_lang = \"en\"\ntarget_lang = \"hi_ng\"\nmax_source_length = 128\nmax_target_length = 128\npadding = \"max_length\"\nnum_epochs = 3\ngradient_accumulation_steps = 2\n\n# Preprocessing function\ndef preprocess_function(examples):\n    inputs = [ex[source_lang].lower().strip() for ex in examples['translation']]\n    targets = [ex[target_lang].lower().strip() for ex in examples['translation']]\n    inputs = [source_prefix + inp for inp in inputs]\n    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n    labels = tokenizer(targets, max_length=max_target_length, padding=padding, truncation=True)\n    labels[\"input_ids\"] = [\n        [(l if l != tokenizer.pad_token_id else -100) for l in label]\n        for label in labels[\"input_ids\"]\n    ]\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\n# Apply preprocessing to datasets\npreprocess_start = time.time()\ntrain_dataset = train_dataset.map(preprocess_function, batched=True, remove_columns=[\"translation\"])\neval_dataset = eval_dataset.map(preprocess_function, batched=True, remove_columns=[\"translation\"])\nprint(f\"Data preprocessed in {time.time() - preprocess_start:.2f} seconds\")\n\n# Data collator\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding=True)\n\n# Create DataLoaders with adjusted batch size\ntrain_dataloader = DataLoader(\n    train_dataset,\n    shuffle=True,\n    collate_fn=data_collator,\n    batch_size=32,  # Adjust batch size based on memory\n)\neval_dataloader = DataLoader(\n    eval_dataset,\n    collate_fn=data_collator,\n    batch_size=32,  # Larger batch size for evaluation\n)\n\n# Optimizer and Scheduler\noptimizer = AdamW(model.parameters(), lr=3e-5)\ntotal_steps = len(train_dataloader) // gradient_accumulation_steps * num_epochs\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=int(0.1 * total_steps),\n    num_training_steps=total_steps,\n)\n\n# Initialize GradScaler for mixed precision\nfrom torch.cuda.amp import autocast, GradScaler\nscaler = GradScaler()\n\n# Metric for BLEU score\nmetric = load_metric('sacrebleu')\n\n# Function to postprocess text\ndef postprocess_text(preds, labels):\n    preds = [pred.strip() for pred in preds]\n    labels = [[label.strip()] for label in labels]\n    return preds, labels\n\n# Training loop with gradient accumulation and mixed precision\ntotal_train_loss = []\ntotal_eval_bleu = []\n\nprint(\"Starting training...\")\nfor epoch in range(num_epochs):\n    epoch_start = time.time()\n    model.train()\n    epoch_train_loss = 0\n    optimizer.zero_grad()\n    for step, batch in enumerate(train_dataloader):\n        batch = {k: v.to(device) for k, v in batch.items()}\n\n        with autocast():\n            outputs = model(**batch)\n            loss = outputs.loss\n            loss = loss.mean() / gradient_accumulation_steps  # Normalize loss\n\n        scaler.scale(loss).backward()\n\n        if (step + 1) % gradient_accumulation_steps == 0 or (step + 1) == len(train_dataloader):\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad()\n            scheduler.step()\n\n        loss_value = loss.item() * gradient_accumulation_steps\n        epoch_train_loss += loss_value\n\n        if step % 1000 == 0:\n            current_loss = epoch_train_loss / (step + 1)\n            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{step}/{len(train_dataloader)}], Loss: {current_loss:.4f}\")\n\n    avg_train_loss = epoch_train_loss / len(train_dataloader)\n    total_train_loss.append(avg_train_loss)\n    print(f\"Epoch [{epoch+1}/{num_epochs}] training completed in {time.time() - epoch_start:.2f} seconds\")\n\n    # Evaluation\n    eval_start = time.time()\n    model.eval()\n    all_preds = []\n    all_labels = []\n    for batch in eval_dataloader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        with torch.no_grad():\n            with autocast():\n                generated_tokens = model.module.generate(\n                    batch[\"input_ids\"],\n                    attention_mask=batch[\"attention_mask\"],\n                    max_length=max_target_length,\n                    num_beams=4,\n                )\n        labels = batch[\"labels\"]\n\n        labels = labels.cpu().numpy()\n        generated_tokens = generated_tokens.cpu().numpy()\n        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n\n        decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n        decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n\n        all_preds.extend(decoded_preds)\n        all_labels.extend(decoded_labels)\n\n    result = metric.compute(predictions=all_preds, references=all_labels)\n    eval_bleu = result[\"score\"]\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Evaluation BLEU Score: {eval_bleu:.2f}\")\n    print(f\"Evaluation completed in {time.time() - eval_start:.2f} seconds\")\n    total_eval_bleu.append(eval_bleu)\n\nprint(\"Training completed.\")\nprint(f\"Total training time: {time.time() - start_time:.2f} seconds\")\n\n# Plot training loss and evaluation BLEU score\nplt.figure(figsize=(10, 6))\nplt.plot(range(1, num_epochs + 1), total_train_loss, label=\"Training Loss\")\nplt.plot(range(1, num_epochs + 1), total_eval_bleu, label=\"Evaluation BLEU Score\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss / BLEU Score\")\nplt.title(\"Training Loss and Evaluation BLEU Score\")\nplt.legend()\nplt.show()\n\n# Generate predictions on the test set\nprint(\"Generating predictions on the test set...\")\nmodel.eval()\nall_preds = []\nall_labels = []\nfor batch in eval_dataloader:\n    batch = {k: v.to(device) for k, v in batch.items()}\n    with torch.no_grad():\n        with autocast():\n            generated_tokens = model.module.generate(\n                batch[\"input_ids\"],\n                attention_mask=batch[\"attention_mask\"],\n                max_length=max_target_length,\n                num_beams=4,\n            )\n    labels = batch[\"labels\"]\n\n    labels = labels.cpu().numpy()\n    generated_tokens = generated_tokens.cpu().numpy()\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n\n    decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n\n    all_preds.extend(decoded_preds)\n    all_labels.extend(decoded_labels)\n\n# Compute individual BLEU scores\nprint(\"Computing BLEU scores for each test sample...\")\nindividual_bleu_scores = [metric.compute(predictions=[pred], references=[[label]])[\"score\"]\n                          for pred, label in zip(all_preds, all_labels)]\n\n# Plot BLEU score distribution\nplt.figure(figsize=(10, 6))\nsns.histplot(individual_bleu_scores, bins=20, kde=True)\nplt.title(\"BLEU Score Distribution on Test Set\")\nplt.xlabel(\"BLEU Score\")\nplt.ylabel(\"Number of Samples\")\nplt.show()\n\n# Print average BLEU score\naverage_bleu = np.mean(individual_bleu_scores)\nprint(f\"Average BLEU score on test set: {average_bleu:.2f}\")\n\n# Sample translations\nprint(\"\\nSample translations from the test set:\")\nfor i in range(5):\n    input_ids = eval_dataset[i][\"input_ids\"]\n    input_text = tokenizer.decode(input_ids, skip_special_tokens=True)\n    print(f\"\\nSource: {input_text.replace(source_prefix, '')}\")\n    print(f\"Reference: {all_labels[i][0]}\")\n    print(f\"Prediction: {all_preds[i]}\")\n\n# Function to translate new sentences\ndef translate_sentence(sentence):\n    input_text = source_prefix + sentence.lower().strip()\n    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n    with torch.no_grad():\n        with autocast():\n            generated_tokens = model.module.generate(\n                input_ids,\n                max_length=max_target_length,\n                num_beams=4,\n            )\n    translated_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n    return translated_text\n\n# Test the model with a custom sentence\ncustom_sentence = \"I was waiting for my bag\"\ntranslated_sentence = translate_sentence(custom_sentence)\nprint(\"\\nCustom Translation:\")\nprint(f\"Input: {custom_sentence}\")\nprint(f\"Translated Output: {translated_sentence}\")\n\n# Optional: Monitor GPU Memory Usage\nprint(f\"Allocated GPU memory: {torch.cuda.memory_allocated() / 1024 ** 3:.2f} GB\")\nprint(f\"Cached GPU memory: {torch.cuda.memory_reserved() / 1024 ** 3:.2f} GB\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T17:44:43.637850Z","iopub.execute_input":"2024-11-12T17:44:43.638266Z","iopub.status.idle":"2024-11-12T20:06:00.055583Z","shell.execute_reply.started":"2024-11-12T17:44:43.638225Z","shell.execute_reply":"2024-11-12T20:06:00.054425Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.1)\nRequirement already satisfied: sacrebleu in /opt/conda/lib/python3.10/site-packages (2.4.3)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (0.2.0)\nRequirement already satisfied: evaluate in /opt/conda/lib/python3.10/site-packages (0.4.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: portalocker in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (2.10.1)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (5.3.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nUsing device: cuda\nNumber of GPUs available: 2\nUsing DataParallel for multi-GPU training.\nDataset loaded in 1.90 seconds\nTraining samples: 170191, Evaluation samples: 18911\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/170191 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12fab0daf2d7498cac2788a10222010a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/18911 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8e098b701bb4cf98dce63968daeed24"}},"metadata":{}},{"name":"stdout","text":"Data preprocessed in 96.49 seconds\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_30/3280749432.py:134: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler()\n","output_type":"stream"},{"name":"stdout","text":"Starting training...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_30/3280749432.py:158: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/3], Step [0/5319], Loss: 7.4982\nEpoch [1/3], Step [1000/5319], Loss: 4.1693\nEpoch [1/3], Step [2000/5319], Loss: 2.8815\nEpoch [1/3], Step [3000/5319], Loss: 2.3100\nEpoch [1/3], Step [4000/5319], Loss: 1.9806\nEpoch [1/3], Step [5000/5319], Loss: 1.7649\nEpoch [1/3] training completed in 1450.00 seconds\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_30/3280749432.py:190: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/3], Evaluation BLEU Score: 48.51\nEvaluation completed in 988.77 seconds\nEpoch [2/3], Step [0/5319], Loss: 0.7809\nEpoch [2/3], Step [1000/5319], Loss: 0.7966\nEpoch [2/3], Step [2000/5319], Loss: 0.7868\nEpoch [2/3], Step [3000/5319], Loss: 0.7724\nEpoch [2/3], Step [4000/5319], Loss: 0.7610\nEpoch [2/3], Step [5000/5319], Loss: 0.7536\nEpoch [2/3] training completed in 1449.85 seconds\nEpoch [2/3], Evaluation BLEU Score: 53.97\nEvaluation completed in 1025.62 seconds\nEpoch [3/3], Step [0/5319], Loss: 0.7828\nEpoch [3/3], Step [1000/5319], Loss: 0.6661\nEpoch [3/3], Step [2000/5319], Loss: 0.6664\nEpoch [3/3], Step [3000/5319], Loss: 0.6599\nEpoch [3/3], Step [4000/5319], Loss: 0.6565\nEpoch [3/3], Step [5000/5319], Loss: 0.6517\nEpoch [3/3] training completed in 1450.04 seconds\nEpoch [3/3], Evaluation BLEU Score: 56.38\nEvaluation completed in 946.92 seconds\nTraining completed.\nTotal training time: 7414.60 seconds\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x600 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAA0kAAAIjCAYAAADWYVDIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABk/klEQVR4nO3dd3gUVd/G8XvTNoUk1BQEQu9FBIGgFBWpIiAWEBVQwQIKYgH0UYoFUBSwgaKAKAjiCwjSBARUBEWKj0h5EKlCQJEkhEDazvtHyDKbbJJN2GST8P1c115kz8yc/e3sJOTOmTljMQzDEAAAAABAkuTl6QIAAAAAoCghJAEAAACACSEJAAAAAEwISQAAAABgQkgCAAAAABNCEgAAAACYEJIAAAAAwISQBAAAAAAmhCQAAAAAMCEkASh2BgwYoKpVq+Zr27Fjx8pisbi3IOTLxo0bZbFYtHHjRk+Xki2LxaKxY8d65LWLw/4BgJKKkATAbSwWi0uPq/WXvgEDBqhUqVKeLqPYmTNnTo7H09atWz1d4hV5//33NWfOHE+X4aB9+/YO+9jPz0/VqlXT4MGDdezYMYd1Mz6fX375Jdv+Dh8+nONnOHHiRPu6VatW1W233ea0n19++UUWi8Wl/XX48GENHDhQNWrUkL+/vyIiItS2bVuNGTPGtZ0A4Krm4+kCAJQcn376qcPzuXPnau3atVna69Wrd0WvM3PmTNlstnxt+5///EejRo26oteHZ4wfP17VqlXL0l6zZk0PVOM+77//vsqXL68BAwY4tLdt21YXLlyQn5+fR+qqVKmSJkyYIElKTk7Wnj17NGPGDK1Zs0Z79+5VYGBgnvvs27evunbtmqW9adOmV1yv2R9//KHrr79eAQEBevDBB1W1alWdPHlSO3bs0KRJkzRu3Di3vh6AkoeQBMBt7rvvPofnW7du1dq1a7O0Z5aYmJinX7h8fX3zVZ8k+fj4yMeHH33FUZcuXdS8eXNPl1FovLy85O/v77HXDw0NzfK9W61aNQ0dOlSbN2/Wrbfemuc+r7vuulx/HrjDlClTlJCQoF27dikqKsph2enTpwv89c3Onz+voKCgQn1NAFeO0+0AFKr27durYcOG2r59u9q2bavAwEA9//zzkqSvvvpK3bp1U8WKFWW1WlWjRg29/PLLSktLc+gj8zVJGafyTJ48WR9++KFq1Kghq9Wq66+/Xtu2bXPY1tk1SRaLRUOHDtXSpUvVsGFDWa1WNWjQQKtXr85S/8aNG9W8eXP5+/urRo0a+uCDD9x+ndOiRYvUrFkzBQQEqHz58rrvvvv0119/OawTExOjgQMHqlKlSrJarYqMjFSPHj10+PBh+zq//PKLOnXqpPLlyysgIEDVqlXTgw8+mOvru/o5ZHyWe/bs0U033aTAwEBdc801ev3117P0efz4cfXs2VNBQUEKCwvTU089paSkpPztICdSUlJUtmxZDRw4MMuy+Ph4+fv765lnnpGUPiry0ksvqVmzZgoNDVVQUJDatGmjDRs25Po62V0P5+wYmD17tm6++WaFhYXJarWqfv36mj59usM6VatW1e+//65NmzbZTz1r3769pOyvSXLl+Mg4tfOvv/5Sz549VapUKVWoUEHPPPNMls8xLyIiIiSpyP+h4eDBg6pUqVKWgCRJYWFhWdpWrVqldu3aKTg4WCEhIbr++us1f/58h3Xyst8PHjyorl27Kjg4WP369ZMk2Ww2TZ06VQ0aNJC/v7/Cw8P1yCOP6OzZs2585wDcpWj/lANQIp05c0ZdunRRnz59dN999yk8PFxS+rUNpUqV0ogRI1SqVCl9++23eumllxQfH6833ngj137nz5+vc+fO6ZFHHpHFYtHrr7+uO+64Q3/++Weuo08//PCDFi9erMcff1zBwcF6++231bt3bx09elTlypWTJO3cuVOdO3dWZGSkxo0bp7S0NI0fP14VKlS48p1yyZw5czRw4EBdf/31mjBhgk6dOqVp06Zp8+bN2rlzp0qXLi1J6t27t37//Xc98cQTqlq1qk6fPq21a9fq6NGj9ucdO3ZUhQoVNGrUKJUuXVqHDx/W4sWLXarB1c/h7Nmz6ty5s+644w7dfffd+vLLLzVy5Eg1atRIXbp0kSRduHBBt9xyi44ePaonn3xSFStW1Keffqpvv/02T/smLi5O//zzj0ObxWJRuXLl5Ovrq169emnx4sX64IMPHE5RW7p0qZKSktSnTx9J6aHpo48+Ut++fTVo0CCdO3dOH3/8sTp16qSff/5Z1157bZ7qys706dPVoEED3X777fLx8dHy5cv1+OOPy2azaciQIZKkqVOn6oknnlCpUqX0wgsvSJL9+8EZV48PSUpLS1OnTp3UsmVLTZ48WevWrdObb76pGjVq6LHHHsu1/rS0NPv+TklJ0d69ezVmzBjVrFlTN9xwQ772SWJiYpbPUJJKly7t1uAVFRWldevW6dtvv9XNN9+c47pz5szRgw8+qAYNGmj06NEqXbq0du7cqdWrV+vee++1r+Pqfk9NTVWnTp104403avLkyfZR8kceecTez5NPPqlDhw7p3Xff1c6dO7V58+YrGiEHUAAMACggQ4YMMTL/mGnXrp0hyZgxY0aW9RMTE7O0PfLII0ZgYKBx8eJFe1v//v2NqKgo+/NDhw4Zkoxy5coZ//77r739q6++MiQZy5cvt7eNGTMmS02SDD8/P+OPP/6wt/3666+GJOOdd96xt3Xv3t0IDAw0/vrrL3vbgQMHDB8fnyx9OtO/f38jKCgo2+XJyclGWFiY0bBhQ+PChQv29q+//tqQZLz00kuGYRjG2bNnDUnGG2+8kW1fS5YsMSQZ27Zty7WuzFz9HDI+y7lz59rbkpKSjIiICKN37972tqlTpxqSjC+++MLedv78eaNmzZqGJGPDhg051jN79mxDktOH1Wq1r7dmzZosn7dhGEbXrl2N6tWr25+npqYaSUlJDuucPXvWCA8PNx588EGHdknGmDFj7M8zH3sZnB1XzvZjp06dHGoxDMNo0KCB0a5duyzrbtiwwWH/uHp8ZNQpyRg/frxDn02bNjWaNWuW5bUyy/hsMz/q1atn/Pnnnw7rZnw+OR1rGd+j2T22bNliXzcqKsro1q2b0362bdtmSDJmz56dY/27d+82AgICDEnGtddeawwbNsxYunSpcf78eYf1YmNjjeDgYKNly5YO+9QwDMNmsxmGkb/9PmrUKIe+vv/+e0OSMW/ePIf21atXO20H4Hmcbgeg0FmtVqenRQUEBNi/PnfunP755x+1adNGiYmJ2rdvX6793nPPPSpTpoz9eZs2bSRJf/75Z67bdujQQTVq1LA/b9y4sUJCQuzbpqWlad26derZs6cqVqxoX69mzZr2EZMr9csvv+j06dN6/PHHHa5F6datm+rWrasVK1ZISt9Pfn5+2rhxY7an6mT8Zfvrr79WSkpKnurIy+dQqlQph2tM/Pz81KJFC4d9vnLlSkVGRurOO++0twUGBmrw4MF5quu9997T2rVrHR6rVq2yL7/55ptVvnx5LVy40N529uxZrV27Vvfcc4+9zdvb2z7SZLPZ9O+//yo1NVXNmzfXjh078lRTTsz7MWMUrF27dvrzzz8VFxeX5/5cPT7MHn30UYfnbdq0cen7QUo/FdC8n6dOnaq4uDh16dJFf//9d57rl6TBgwdn+QzXrl2r+vXr56u/7DRo0EC7du3Sfffdp8OHD2vatGnq2bOnwsPDNXPmTPt6a9eu1blz5zRq1Kgs139lnD6Zn/2eeaRu0aJFCg0N1a233qp//vnH/mjWrJlKlSrl0qmeAAoXp9sBKHTXXHON0xm7fv/9d/3nP//Rt99+q/j4eIdlrvxSWaVKFYfnGYHJlXP+M2+bsX3GtqdPn9aFCxeczqTmrtnVjhw5IkmqU6dOlmV169bVDz/8ICk9ZE6aNElPP/20wsPD1apVK91222164IEH7NeMtGvXTr1799a4ceM0ZcoUtW/fXj179tS9994rq9WaYx15+RwqVaqU5VqcMmXK6L///a/D+6pZs2aW9Zy9z5y0aNEix4kbfHx81Lt3b82fP19JSUmyWq1avHixUlJSHEKSJH3yySd68803tW/fPocQ6Wz2vPzavHmzxowZoy1btigxMdFhWVxcnEJDQ/PUn6vHRwZ/f/8sp4Kaj+ncBAUFqUOHDvbnnTt31o033qjmzZtr4sSJevPNN/NUvyTVqlXLoc/8cuUawNq1a+vTTz9VWlqa9uzZo6+//lqvv/66Bg8erGrVqqlDhw46ePCgJKlhw4bZ9pPX/e7j46NKlSo5tB04cEBxcXFOr4eSCn8yCQC5IyQBKHTmv7BniI2NVbt27RQSEqLx48fb722yY8cOjRw50qUpv729vZ22G4ZRoNt6wvDhw9W9e3ctXbpUa9as0YsvvqgJEybo22+/VdOmTWWxWPTll19q69atWr58udasWaMHH3xQb775prZu3Zrt/Zry+jkUtf3Wp08fffDBB1q1apV69uypL774QnXr1lWTJk3s63z22WcaMGCAevbsqWeffVZhYWHy9vbWhAkT7L80Zye7X84zT4Zw8OBB3XLLLapbt67eeustVa5cWX5+flq5cqWmTJmS7yns8yK7z+ZKZEx28d1337m97wz+/v66cOGC02UZYTMvs/55e3urUaNGatSokaKjo3XTTTdp3rx5bglrzlitVnl5OZ6oY7PZFBYWpnnz5jndxp3XNQJwD0ISgCJh48aNOnPmjBYvXqy2bdva2w8dOuTBqi4LCwuTv7+//vjjjyzLnLXlR8ZMXPv3789ysfn+/fuzzNRVo0YNPf3003r66ad14MABXXvttXrzzTf12Wef2ddp1aqVWrVqpVdffVXz589Xv379tGDBAj388MNOayiIzyEqKkq7d++WYRgOIWP//v357jM7bdu2VWRkpBYuXKgbb7xR3377rX1ChAxffvmlqlevrsWLFzvU48pNRsuUKaPY2Ngs7RmjDRmWL1+upKQkLVu2zGGU0tlpVa7OjJjX46OgpKWlKSEhocD6j4qK0p49e5wuyzhm8vteM0YiT548KUn2U2x3796d7YiwO/Z7jRo1tG7dOt1www1O/0gEoOjhmiQARULGX73NIxDJycl6//33PVWSA29vb3Xo0EFLly7ViRMn7O1//PGHw3UxV6J58+YKCwvTjBkzHKbHXrVqlfbu3atu3bpJSv9r+sWLFx22rVGjhoKDg+3bnT17NstoTsasbTlNvV0Qn0PXrl114sQJffnll/a2xMREffjhh/nuMzteXl668847tXz5cn366adKTU3Ncqqds/f4008/acuWLbn2X6NGDcXFxTmcTnjy5EktWbIk19eIi4vT7Nmzs/QZFBTkNHhl5urxUZA2bNighIQEh5E5d+vatauOHz+upUuXOrQnJSXpo48+UlhYmK677roc+/j++++dXou3cuVKSZdPnevYsaOCg4M1YcKELN9TGZ+dO/b73XffrbS0NL388stZlqWmprr0+QMoXIwkASgSWrdurTJlyqh///568sknZbFY9Omnnxap093Gjh2rb775RjfccIMee+wxpaWl6d1331XDhg21a9cul/pISUnRK6+8kqW9bNmyevzxxzVp0iQNHDhQ7dq1U9++fe1TDVetWlVPPfWUJOl///ufbrnlFt19992qX7++fHx8tGTJEp06dco+zfUnn3yi999/X7169VKNGjV07tw5zZw5UyEhIeratWu29RXE5zBo0CC9++67euCBB7R9+3ZFRkbq008/zdMNhKX0X0qdTeDRunVrVa9e3f78nnvu0TvvvKMxY8aoUaNGqlevnsP6t912mxYvXqxevXqpW7duOnTokGbMmKH69evnOkLSp08fjRw5Ur169dKTTz6pxMRETZ8+XbVr13aY9KFjx47y8/NT9+7d9cgjjyghIUEzZ85UWFiYfRQjQ7NmzTR9+nS98sorqlmzpsLCwpxOW+3r6+vS8eEucXFx9lHJ1NRU7d+/X9OnT1dAQIBGjRqVZf1Zs2Y5vbfYsGHD7F/v2LHDYaQzQ40aNRQdHS0pfXKHWbNm6a677tKDDz6opk2b6syZM1q4cKF2796tuXPnOr2m0WzSpEnavn277rjjDjVu3Nj+2nPnzlXZsmU1fPhwSVJISIimTJmihx9+WNdff73uvfdelSlTRr/++qsSExP1ySefuGW/t2vXTo888ogmTJigXbt2qWPHjvL19dWBAwe0aNEiTZs2zWFiEwBFgGcm1QNwNchuCvAGDRo4XX/z5s1Gq1atjICAAKNixYrGc889Z5/W2TxNdHZTgDubEluZpnDObgrwIUOGZNk2KirK6N+/v0Pb+vXrjaZNmxp+fn5GjRo1jI8++sh4+umnDX9//2z2wmUZ0wM7e9SoUcO+3sKFC42mTZsaVqvVKFu2rNGvXz/j+PHj9uX//POPMWTIEKNu3bpGUFCQERoaarRs2dJhiu0dO3YYffv2NapUqWJYrVYjLCzMuO2224xffvkl1zpd/Ryy+yydTZN95MgR4/bbbzcCAwON8uXLG8OGDbNPf3wlU4DLyXTQNpvNqFy5siHJeOWVV7L0Z7PZjNdee82IiooyrFar0bRpU+Prr792Wnfm48cwDOObb74xGjZsaPj5+Rl16tQxPvvsM6fH1bJly4zGjRsb/v7+RtWqVY1JkyYZs2bNMiQZhw4dsq8XExNjdOvWzQgODjYk2acDzzwFeIbcjg/DyH66eWd1OpN5CnCLxWKULVvWuP32243t27c7rJvb53Ps2LFcpwDP/H129uxZ46mnnjKqVatm+Pr6GiEhIcZNN91krFq1KtfaDSP9GB4yZIjRsGFDIzQ01PD19TWqVKliDBgwwDh48GCW9ZctW2a0bt3aCAgIMEJCQowWLVoYn3/+ucM6V7LfM3z44YdGs2bNjICAACM4ONho1KiR8dxzzxknTpxw6X0BKDwWwyhCf6YFgGKoZ8+e+v3333XgwAFPlwIAANyAa5IAIA8yz7p14MABrVy5Uu3bt/dMQQAAwO0YSQKAPIiMjNSAAQNUvXp1HTlyRNOnT1dSUpJ27typWrVqebo8AADgBkzcAAB50LlzZ33++eeKiYmR1WpVdHS0XnvtNQISAAAlCCNJAAAAAGDCNUkAAAAAYEJIAgAAAACTEn9Nks1m04kTJxQcHCyLxeLpcgAAAAB4iGEYOnfunCpWrCgvr+zHi0p8SDpx4oQqV67s6TIAAAAAFBHHjh1TpUqVsl1e4kNScHCwpPQdERIS4uFqAAAAAHhKfHy8KleubM8I2SnxISnjFLuQkBBCEgAAAIBcL8Nh4gYAAAAAMCEkAQAAAIAJIQkAAAAATEr8NUmuMAxDqampSktL83QpQJHn7e0tHx8fptQHAAAl1lUfkpKTk3Xy5EklJiZ6uhSg2AgMDFRkZKT8/Pw8XQoAAIDbXdUhyWaz6dChQ/L29lbFihXl5+fHX8eBHBiGoeTkZP399986dOiQatWqleON2AAAAIqjqzokJScny2azqXLlygoMDPR0OUCxEBAQIF9fXx05ckTJycny9/f3dEkAAABuxZ+AJf4SDuQR3zMAAKAk4zcdAAAAADAhJAEAAACACSEJkqSqVatq6tSpLq+/ceNGWSwWxcbGFlhNAAAAgCcQkooZi8WS42Ps2LH56nfbtm0aPHiwy+u3bt1aJ0+eVGhoaL5ez1WEMQAAABS2q3p2u+Lo5MmT9q8XLlyol156Sfv377e3lSpVyv61YRhKS0uTj0/uH3OFChXyVIefn58iIiLytA0AAABQHDCSZGIYhhKTUz3yMAzDpRojIiLsj9DQUFksFvvzffv2KTg4WKtWrVKzZs1ktVr1ww8/6ODBg+rRo4fCw8NVqlQpXX/99Vq3bp1Dv5lPt7NYLProo4/Uq1cvBQYGqlatWlq2bJl9eeYRnjlz5qh06dJas2aN6tWrp1KlSqlz584OoS41NVVPPvmkSpcurXLlymnkyJHq37+/evbsme/P7OzZs3rggQdUpkwZBQYGqkuXLjpw4IB9+ZEjR9S9e3eVKVNGQUFBatCggVauXGnftl+/fqpQoYICAgJUq1YtzZ49O9+1AAAAoGRgJMnkQkqa6r+0xiOvvWd8JwX6uefjGDVqlCZPnqzq1aurTJkyOnbsmLp27apXX31VVqtVc+fOVffu3bV//35VqVIl237GjRun119/XW+88Ybeeecd9evXT0eOHFHZsmWdrp+YmKjJkyfr008/lZeXl+677z4988wzmjdvniRp0qRJmjdvnmbPnq169epp2rRpWrp0qW666aZ8v9cBAwbowIEDWrZsmUJCQjRy5Eh17dpVe/bska+vr4YMGaLk5GR99913CgoK0p49e+yjbS+++KL27NmjVatWqXz58vrjjz904cKFfNcCAACAkoGQVAKNHz9et956q/152bJl1aRJE/vzl19+WUuWLNGyZcs0dOjQbPsZMGCA+vbtK0l67bXX9Pbbb+vnn39W586dna6fkpKiGTNmqEaNGpKkoUOHavz48fbl77zzjkaPHq1evXpJkt599137qE5+ZISjzZs3q3Xr1pKkefPmqXLlylq6dKnuuusuHT16VL1791ajRo0kSdWrV7dvf/ToUTVt2lTNmzeXlD6aBgAAABCSTAJ8vbVnfCePvba7ZPzSnyEhIUFjx47VihUrdPLkSaWmpurChQs6evRojv00btzY/nVQUJBCQkJ0+vTpbNcPDAy0ByRJioyMtK8fFxenU6dOqUWLFvbl3t7eatasmWw2W57eX4a9e/fKx8dHLVu2tLeVK1dOderU0d69eyVJTz75pB577DF988036tChg3r37m1/X4899ph69+6tHTt2qGPHjurZs6c9bAEAACCPbGlScoKUlGD695yUdE4KKCtVvcHTFbqMkGRisVjcdsqbJwUFBTk8f+aZZ7R27VpNnjxZNWvWVEBAgO68804lJyfn2I+vr6/Dc4vFkmOgcba+q9daFZSHH35YnTp10ooVK/TNN99owoQJevPNN/XEE0+oS5cuOnLkiFauXKm1a9fqlltu0ZAhQzR58mSP1gwAAFBoUpPTQ0zyOcdwkxSfKfBcCjs5taWcz/51anUkJKFo2bx5swYMGGA/zS0hIUGHDx8u1BpCQ0MVHh6ubdu2qW3btpKktLQ07dixQ9dee22++qxXr55SU1P1008/2UeAzpw5o/3796t+/fr29SpXrqxHH31Ujz76qEaPHq2ZM2fqiSeekJQ+q1///v3Vv39/tWnTRs8++ywhCQAAFF2GISWfzzRSk2AKK6bQ4jT8ZGpLy/mP5vni5SNZgyW/YMlaSvIrJZWv7f7XKUCEpKtArVq1tHjxYnXv3l0Wi0Uvvvhivk9xuxJPPPGEJkyYoJo1a6pu3bp65513dPbsWVkslly3/e233xQcHGx/brFY1KRJE/Xo0UODBg3SBx98oODgYI0aNUrXXHONevToIUkaPny4unTpotq1a+vs2bPasGGD6tWrJ0l66aWX1KxZMzVo0EBJSUn6+uuv7csAAADcJi01a1jJOA3NPCpjH53JoS05QTIK4Pc438D0MJMRaqzBl4JOTm2XQpC97dK/PlbJhd/vijJC0lXgrbfe0oMPPqjWrVurfPnyGjlypOLj4wu9jpEjRyomJkYPPPCAvL29NXjwYHXq1Ene3rlfj5Ux+pTB29tbqampmj17toYNG6bbbrtNycnJatu2rVauXGk/9S8tLU1DhgzR8ePHFRISos6dO2vKlCmS0u/1NHr0aB0+fFgBAQFq06aNFixY4P43DgAAihfDkFIvOo7UuHrKWfKlU9XMgSi1AGbPtXg5jtRkF2Qyj+hYS0nWEMdt/EpJ3sQCM4vh6YtGClh8fLxCQ0MVFxenkJAQh2UXL17UoUOHVK1aNfn7+3uowquXzWZTvXr1dPfdd+vll1/2dDnIA753AABFjs12eaTFpdPQchm9saW6v0Zvv6yjLtZgx4Dj0BacffjxDSz2ozWekFM2MCMyotAcOXJE33zzjdq1a6ekpCS9++67OnTokO69915PlwYAADwhNdnF62hcmEQgOaFgavTLZaQmT6eh+RVMjXA7QhIKjZeXl+bMmaNnnnlGhmGoYcOGWrduHdcBAQBQXBiGlJKYwzUzOZya5mxEJy3J/TVavF0MMtmFm2DH09C8vNxfI4o8QhIKTeXKlbV582ZPlwEAwNXFlpb7NTMuTSJwab2CmDTAx9+1kRhXTk3z8ec0NFwxQhIAAEBRYhhSapL7TkNLSSyAIi1OQouTCQGctjkZvWHSABQxHJEAAABXymZLv5FmtmEl8yQCubTZUtxfo5dvDhMC5DJJgN+lsJPxtW8gp6GhRCMkAQCAq1NaSj5OQ8uuLUFSAUwY7BuYy2lorpyalnEamtX99QElFCEJAAAUD4YhpVzIw403M08ikKkt9aL7a7Tfu8bZ/WtClLfT0EpJXrnfSxCA+xGSAABAwbGlZRqpceU6mhxOQzPS3F+jt/UKT0MztfkGMGkAUAIQkgAAgKPUJBdvxpl5EgEnbSnnC6ZGv8ynnLlyr5psTk3z9i2YGgEUW4QkOHX48GFVq1ZNO3fu1LXXXlugrzVnzhwNHz5csbGxBfo6AFBiGYaUfD4P0znncBpa0rkCmjTAx/kpZ662mU9N8w1i0gAABYqQVAwNGDBAn3zySZb2Tp06afXq1R6oyHVVq1bV8OHDNXz4cHvbPffco65duxb4a7dv316bNm2yPw8LC1Pbtm01efJkRUVFSco9HM6ZM0cDBw7M0m61WnXx4kX761x77bWaOnVqlm1zC4NLlizRpEmTtHfvXtlsNlWpUkW33nprlr4AlABpqTnfeNPpJALZtZ1TgUwa4BOQ/3vVZB7Z8bFyGhqAYoOQVEx17txZs2fPdmizWovnrDUBAQEKCAgolNcaNGiQxo8fL8MwdOTIEQ0fPlz33Xefvv/+e5f7CAkJ0f79+x3aLG74j3/9+vW655579Oqrr+r222+XxWLRnj17tHbt2ivuOztpaWmyWCzy4i+yQO4MI/1Cf5dOQ3Nh9KYgJg2QJfdrZlw9Nc2vFPeuAXDV4qefmWEU0A3XXOAbmKe/sFmtVkVERDhddu+99yotLU0LFy60t6WkpCgyMlJvvfWWHnjgAa1evVqvvPKKdu/eLW9vb0VHR2vatGmqUaOG0z6djYIsXbpUvXr1kmGk//Xy4MGDGjFihLZu3arz58+rXr16mjBhgjp06CApfYTlyJEjeuqpp/TUU09JkgzDcNr39OnTNXnyZB07dkzVqlXTf/7zH91///325RaLRTNnztSKFSu0Zs0aXXPNNXrzzTd1++2357jfAgMD7fstMjJSQ4cO1SOPPJLjNplZLJZs9/2VWL58uW644QY9++yz9rbatWurZ8+eWdYbP368fvvtN5UqVUpt2rTRkiVLJElnz57VsGHDtHz5ciUlJaldu3Z6++23VatWLUmXP8e5c+dq1KhR+t///qc//vhDkZGReuGFF/T5558rNjZWDRs21KRJk9S+fXu3v0/AI5LOSedipIvxLoze5NBWIJMG+OUhyDg7Nc00k1oe/y8BADhHSDJLSZReq+iZ137+hOQX5Jau+vXrp7vuuksJCQkqVaqUJGnNmjVKTExUr169JEnnz5/XiBEj1LhxYyUkJOill15Sr169tGvXrnyPKiQkJKhr16569dVXZbVaNXfuXHXv3l379+9XlSpVtHjxYjVp0kSDBw/WoEGDsu1nyZIlGjZsmKZOnaoOHTro66+/1sCBA1WpUiXddNNN9vXGjRun119/XW+88Ybeeecd9evXT0eOHFHZsmVdqvfff//VF198oZYtW+br/bpbRESE5s+fr927d6thw4ZO11mxYoV69eqlF154QXPnzlVycrJWrlxpXz5gwAAdOHBAy5YtU0hIiEaOHKmuXbtqz5498vVNvzA5MTFRkyZN0kcffaRy5copLCxMQ4cO1Z49e7RgwQJVrFhRS5YsUefOnfXbb7/ZAxZQZKWlSudOSnHHLz2OXf46/q/05xfj3PuavkHZnIbmwr1qspyG5ufe2gAAV4yQVEx9/fXX9gCU4fnnn9fzzz+vTp06KSgoSEuWLLGPvsyfP1+33367goODJUm9e/d22HbWrFmqUKGC9uzZk+0v6Llp0qSJmjRpYn/+8ssva8mSJVq2bJmGDh2qsmXLytvbW8HBwTmOxEyePFkDBgzQ448/Lkn20anJkyc7hKQBAwaob9++kqTXXntNb7/9tn7++Wd17tw5277ff/99ffTRRzIMQ4mJiapdu7bWrFmTp/cZFxeXZd+3adNGq1atylM/mT3xxBP6/vvv1ahRI0VFRalVq1bq2LGj+vXrZz+V8tVXX1WfPn00btw4+3YZ+zwjHG3evFmtW7eWJM2bN0+VK1fW0qVLddddd0lKH1V8//337dsdPXpUs2fP1tGjR1WxYvofCZ555hmtXr1as2fP1muvvXZF7wu4IoYhXYw1BaBMISjuL+ncCcmw5d6XNeTSI9P9a3K9V03m09CCuHcNAJRwhCQz38D0ER1PvXYe3HTTTZo+fbpDW8YIio+Pj+6++27NmzdP999/v86fP6+vvvpKCxYssK974MABvfTSS/rpp5/0zz//yGZL/wXj6NGj+Q5JCQkJGjt2rFasWKGTJ08qNTVVFy5c0NGjR/PUz969ezV48GCHthtuuEHTpk1zaGvcuLH966CgIIWEhOj06dM59t2vXz+98MILkqRTp07ptddeU8eOHbV9+3Z7gMxNcHCwduzY4dDmjmuqgoKCtGLFCh08eFAbNmzQ1q1b9fTTT2vatGnasmWLAgMDtWvXrmxH4fbu3SsfHx+HkbFy5cqpTp062rt3r73Nz8/PYd/99ttvSktLU+3atR36S0pKUrly5a74fQE5Sk2+NNrjJARltCcn5N6Pl68Ueo0UWlkKrSSFXJP+b8bz0GvSQw4AAC4gJJlZLG475a2gBQUFqWbNmtku79evn9q1a6fTp09r7dq1CggIcBhh6d69u6KiojRz5kxVrFhRNptNDRs2VHJystP+vLy87NceZUhJcZwi9plnntHatWs1efJk1axZUwEBAbrzzjuz7fNKZZw+lsFisdjDXnZCQ0Pt+61mzZr6+OOPFRkZqYULF+rhhx926XW9vLxy3PchISGKi8t6ak9sbKxCQ0Nz7b9GjRqqUaOGHn74Yb3wwguqXbu2Fi5cqIEDB7oljAUEBDhMNJGQkCBvb29t375d3t6Ofx3PPGIG5IlhSIlnMo38mEeC/pISTsmlWdkCy18KOxnBJ1MICgpjSmgAgNsQkkqo1q1bq3Llylq4cKFWrVqlu+66yx4qzpw5o/3792vmzJlq06aNJOmHH37Isb8KFSro3LlzOn/+vIKC0oPkrl27HNbZvHmzBgwYYL/uKSEhQYcPH3ZYx8/PT2lpOV/4XK9ePW3evFn9+/d36Lt+/fq5vu+8yggFFy5ccFufderU0TfffJOlfceOHVlGa3JTtWpVBQYG6vz59JsxNm7cWOvXr3c6DXm9evWUmpqqn376yX66XcZnndO+a9q0qdLS0nT69Gn78QC4JOVCetBxFoIyRoFcmcHNx98UgC4FH4eRoGsk38KZARMAAImQVGwlJSUpJibGoc3Hx0fly5e3P7/33ns1Y8YM/e9//9OGDRvs7WXKlFG5cuX04YcfKjIyUkePHtWoUaNyfL2WLVsqMDBQzz//vJ588kn99NNPmjNnjsM6tWrV0uLFi9W9e3dZLBa9+OKLWUZ2qlatqu+++059+vSR1Wp1qDfDs88+q7vvvltNmzZVhw4dtHz5ci1evFjr1q1zdfdkKzEx0b7fTp06pZdffln+/v7q2LGjw3qZp/iWpAYNGkhKn5Ev876X0u+75OXlpccee0zvvvuunnzyST388MOyWq1asWKFPv/8cy1fvjzb2saOHavExER17dpVUVFRio2N1dtvv62UlBTdeuutkqQxY8bolltuUY0aNdSnTx+lpqZq5cqVGjlypGrVqqUePXpo0KBB+uCDDxQcHKxRo0bpmmuuUY8ePbJ93dq1a6tfv3564IEH9Oabb6pp06b6+++/tX79ejVu3FjdunXLfcei5LHZpPOnnVwDZHok/uNaX6UiMoWgTIEosBwzsgEAihRCUjG1evVqRUZGOrTVqVNH+/btsz/v16+fXn31VUVFRemGG26wt3t5eWnBggV68skn1bBhQ9WpU0dvv/12jtM9ly1bVp999pmeffZZzZw5U7fccovGjh3rcO3QW2+9pQcffFCtW7dW+fLlNXLkSMXHxzv0M378eD3yyCOqUaOGkpKSspzCJ0k9e/bUtGnTNHnyZA0bNkzVqlXT7Nmz3TId9cyZMzVz5kxJ6WGxcePGWrlyperUqeOwXp8+fbJse+zYMUlSfHx8ln0vSSdPnlRERISqV6+u7777Ti+88II6dOig5ORk1a1bV4sWLcpxUol27drpvffe0wMPPKBTp06pTJkyatq0qb755ht7fe3bt9eiRYv08ssva+LEiQoJCVHbtm3tfcyePVvDhg3TbbfdpuTkZLVt21YrV67McmpiZrNnz9Yrr7yip59+Wn/99ZfKly+vVq1a6bbbbstxOxRjSecujQI5CUHxl06Fs6Xk3o9vkFS68uXQE5IpBIVUTL+JKAAAxYjFcPZbagkSHx+v0NBQxcXFKSQkxGHZxYsXdejQIVWrVk3+/v4eqhAofvjeKeLSUqWEmOxng4s7lj5jXG4sXlJwxRxGgSpJ/qUZBQIAFBs5ZQMzRpIAoDgxjPR7/jgNQJdmhIs/4dpNT/1DTbO/ORkJCo6UvPlvAgBw9eF/PwAoSlKT0+/7k+0o0HEp+Vzu/Xj5XJr8wByCMk2R7Z/9X9AAALiaEZIAoLAYhpT4b6bgk+meQOdi5NqU2OUy3QeokmMoKhXGDU8BAMgnQhIAuEvKxUthx0kIyhgFSnVhunlva6Z7AmUaCQq5RvLL2w2oAQCA6whJktMZ1gBk76r8nrHZpPN/OzkFznRPoPN/u9ZXqfCsIch8X6Cg8kyGAACAB13VISljWuTExEQFBHCjQsBViYmJkpTr1OLFSlKCk1GgTBMipCXn3o9vUNb7ANlHgS6FIabEBgCgSLuqQ5K3t7dKly6t06dPS5ICAwNl4a+3QLYMw1BiYqJOnz6t0qVLy9u7mFzzYktLv9Yn23sCHZcunM29H4tX+oxvmUOQfRSokhRQhlEgAACKuas6JElSRESEJNmDEoDclS5d2v69UyRkOyX2X5dHgVyZEtsa6uReQKaRoOBIybsEjZ4BAACnrvqQZLFYFBkZqbCwMKWkuHB3eeAq5+vrW7gjSGkp6ff9yW42uLjjUlJ87v14+UghFbPeF8g+EnRN+n2DAADAVe+qD0kZvL29i8+pQ0BJYRjpp7nlNBvcuZNyaUrsgLJOZoMzPUqFMyU2AABwCSEJQMGxT4l9PPuRoJTE3Pvxtl6e+CBzCAq5dCqcX1DBvx8AAHBVICQByB+bTUr8x8lscKaRoPMuXusXFJbNfYEuPQLLS15eBft+AAAALiEkAXAu+fylU96chKD4v9KXpSXl3o9vYDYTIVyaDjvkGsnXv+DfDwAAgIsIScDVyJYmJZxyPiV2xuPCvy50ZHE+Jbb5OVNiAwCAYoaQBJREF+OzmRL70n2B4k9IttTc+7GGZD8ldsg16bPFMSU2AAAoYQhJQHGTlpI+41tO9wVKisu9H4u3401Qnd0XiCmxAQDAVYiQBBQl9imxc5gN7txJybDl3ldAmWwmQqjMlNgAAAA5ICQBhSk1KfspsTNGgVLO596Pt59pFMhZCGJKbAAAgPwiJAHuYhjS+WymxM4IRgmnXOsrqEIOU2JXZkpsAACAAuTRkDR27FiNGzfOoa1OnTrat2+fJOnixYt6+umntWDBAiUlJalTp056//33FR4e7olycbVLTrwUdpzdF+hSCHJlSmyfgJxng2NKbAAAAI/y+EhSgwYNtG7dOvtzH5/LJT311FNasWKFFi1apNDQUA0dOlR33HGHNm/e7IlSUZLZbNlPiR1/6d/EMy50ZJGCI3IIQZWZEhsAAKCI83hI8vHxUURERJb2uLg4ffzxx5o/f75uvvlmSdLs2bNVr149bd26Va1atSrsUlGcJZ3LYTa4Y5emxE7JvR+/4Bxmg6uUfs8gH7+Cfz8AAAAoMB4PSQcOHFDFihXl7++v6OhoTZgwQVWqVNH27duVkpKiDh062NetW7euqlSpoi1btmQbkpKSkpSUdPmUp/j4+AJ/D/CwtNTsp8TOOD3uoqtTYlfMOQQxJTYAAECJ59GQ1LJlS82ZM0d16tTRyZMnNW7cOLVp00a7d+9WTEyM/Pz8VLp0aYdtwsPDFRMTk22fEyZMyHKdE4oxw5AuxuY8G9y5E65PiR2SOQCZQlBwBFNiAwAAwLMhqUuXLvavGzdurJYtWyoqKkpffPGFAgIC8tXn6NGjNWLECPvz+Ph4Va5c+YprRQFJTXY+Jba5LTkh9368/S6NAmWeCe5SCAq5RrKWKvj3AwAAgGLP46fbmZUuXVq1a9fWH3/8oVtvvVXJycmKjY11GE06deqU02uYMlitVlmt1kKoFrkyjPTJDnKaDS7hlCQj976CKmRzX6BLXwdVYEpsAAAAuEWRCkkJCQk6ePCg7r//fjVr1ky+vr5av369evfuLUnav3+/jh49qujoaA9XCklSyoXLEx84mw0u7riUejH3fuxTYmcTgkIqSr75G1kEAAAA8sqjIemZZ55R9+7dFRUVpRMnTmjMmDHy9vZW3759FRoaqoceekgjRoxQ2bJlFRISoieeeELR0dHMbFcYbDbp/GnnU2JnPBL/caGjS1Ni20eBnISgwLJMiQ0AAIAiw6Mh6fjx4+rbt6/OnDmjChUq6MYbb9TWrVtVoUIFSdKUKVPk5eWl3r17O9xMFm6QdO7yKW/OZoOL+8vFKbFLXQo92YSg4IpMiQ0AAIBixWIYhgsXhBRf8fHxCg0NVVxcnEJCQjxdTuFIS5USYnK+L9DF2Nz7MU+Jnd1IkH8oo0AAAAAoFlzNBkXqmiS4wDDS7/njNABdGgmKPyEZabn35V8651GgUhGSN4cIAAAAri78BlzUpCan3/cn21Gg41Lyudz78fK9FH4qZzMKdI1kDS749wMAAAAUM4SkwmQYUuK/mYLPMdO1QMelczFyaUrswPJO7gdknhI7jCmxAQAAgHwgJBUWw5AmRaWfKpcbH//LoSfE2Y1RK0p+gQVfMwAAAHAVIiQVFotFCiiTHpJKRWQzCpQxJXY5JkMAAAAAPISQVJgeXJMelHysnq4EAAAAQDYISYUpOMLTFQAAAADIBVf2AwAAAIAJIQkAAAAATAhJAAAAAGBCSAIAAAAAE0ISAAAAAJgQkgAAAADAhJAEAAAAACaEJAAAAAAwISQBAAAAgAkhCQAAAABMCEkAAAAAYEJIAgAAAAATQhIAAAAAmBCSAAAAAMCEkAQAAAAAJoQkAAAAADAhJAEAAACACSEJAAAAAEwISQAAAABgQkgCAAAAABNCEgAAAACYEJIAAAAAwISQBAAAAAAmhCQAAAAAMCEkAQAAAIAJIQkAAAAATAhJAAAAAGBCSAIAAAAAE0ISAAAAAJgQkgAAAADAhJAEAAAAACaEJAAAAAAwISQBAAAAgAkhCQAAAABMCEkAAAAAYEJIAgAAAAATQhIAAAAAmBCSAAAAAMCEkAQAAAAAJoQkAAAAADAhJAEAAACACSEJAAAAAEwISQAAAABgQkgCAAAAABNCEgAAAACYEJIAAAAAwISQBAAAAAAmhCQAAAAAMCEkAQAAAIAJIQkAAAAATAhJAAAAAGBCSAIAAAAAE0ISAAAAAJgQkgAAAADAhJAEAAAAACaEJAAAAAAwISQBAAAAgEmRCUkTJ06UxWLR8OHD7W0XL17UkCFDVK5cOZUqVUq9e/fWqVOnPFckAAAAgBKvSISkbdu26YMPPlDjxo0d2p966iktX75cixYt0qZNm3TixAndcccdHqoSAAAAwNXA4yEpISFB/fr108yZM1WmTBl7e1xcnD7++GO99dZbuvnmm9WsWTPNnj1bP/74o7Zu3erBigEAAACUZB4PSUOGDFG3bt3UoUMHh/bt27crJSXFob1u3bqqUqWKtmzZkm1/SUlJio+Pd3gAAAAAgKt8PPniCxYs0I4dO7Rt27Ysy2JiYuTn56fSpUs7tIeHhysmJibbPidMmKBx48a5u1QAAAAAVwmPjSQdO3ZMw4YN07x58+Tv7++2fkePHq24uDj749ixY27rGwAAAEDJ57GQtH37dp0+fVrXXXedfHx85OPjo02bNuntt9+Wj4+PwsPDlZycrNjYWIftTp06pYiIiGz7tVqtCgkJcXgAAAAAgKs8drrdLbfcot9++82hbeDAgapbt65GjhypypUry9fXV+vXr1fv3r0lSfv379fRo0cVHR3tiZIBAAAAXAU8FpKCg4PVsGFDh7agoCCVK1fO3v7QQw9pxIgRKlu2rEJCQvTEE08oOjparVq18kTJAAAAAK4CHp24ITdTpkyRl5eXevfuraSkJHXq1Envv/++p8sCAAAAUIJZDMMwPF1EQYqPj1doaKji4uK4PgkAAAC4irmaDTx+nyQAAAAAKEoISQAAAABgQkgCAAAAABNCEgAAAACYEJIAAAAAwISQBAAAAAAmhCQAAAAAMCEkAQAAAIAJIQkAAAAATAhJAAAAAGBCSAIAAAAAE0ISAAAAAJgQkgAAAADAhJAEAAAAACaEJAAAAAAwISQBAAAAgAkhCQAAAABMCEkAAAAAYEJIAgAAAAATQhIAAAAAmOQ7JKWmpmrdunX64IMPdO7cOUnSiRMnlJCQ4LbiAAAAAKCw+eRnoyNHjqhz5846evSokpKSdOuttyo4OFiTJk1SUlKSZsyY4e46AQAAAKBQ5GskadiwYWrevLnOnj2rgIAAe3uvXr20fv16txUHAAAAAIUtXyNJ33//vX788Uf5+fk5tFetWlV//fWXWwoDAAAAAE/I10iSzWZTWlpalvbjx48rODj4iosCAAAAAE/JV0jq2LGjpk6dan9usViUkJCgMWPGqGvXru6qDQAAAAAKncUwDCOvGx07dkydO3eWYRg6cOCAmjdvrgMHDqh8+fL67rvvFBYWVhC15kt8fLxCQ0MVFxenkJAQT5cDAAAAwENczQb5CklS+hTgCxcu1K+//qqEhARdd9116tevn8NEDkUBIQkAAACAVIAhKSUlRXXr1tXXX3+tevXqXXGhBY2QBAAAAEByPRvk+ZokX19fXbx48YqKAwAAAICiKl8TNwwZMkSTJk1Samqqu+sBAAAAAI/K132Stm3bpvXr1+ubb75Ro0aNFBQU5LB88eLFbikOAAAAAApbvkJS6dKl1bt3b3fXAgAAAAAel6+QNHv2bHfXAQAAAABFQr5CUoa///5b+/fvlyTVqVNHFSpUcEtRAAAAAOAp+Zq44fz583rwwQcVGRmptm3bqm3btqpYsaIeeughJSYmurtGAAAAACg0+QpJI0aM0KZNm7R8+XLFxsYqNjZWX331lTZt2qSnn37a3TUCAAAAQKHJ881kJal8+fL68ssv1b59e4f2DRs26O6779bff//trvquGDeTBQAAACAV4M1kJSkxMVHh4eFZ2sPCwjjdDgAAAECxlq+QFB0drTFjxujixYv2tgsXLmjcuHGKjo52W3EAAAAAUNjyNbvdtGnT1KlTJ1WqVElNmjSRJP3666/y9/fXmjVr3FogAAAAABSmfF2TJKWfcjdv3jzt27dPklSvXj3169dPAQEBbi3wSnFNEgAAAADJ9WyQ7/skBQYGatCgQfndHAAAAACKpHxdkzRhwgTNmjUrS/usWbM0adKkKy4KAAAAADwlXyHpgw8+UN26dbO0N2jQQDNmzLjiogAAAADAU/IVkmJiYhQZGZmlvUKFCjp58uQVFwUAAAAAnpKvkFS5cmVt3rw5S/vmzZtVsWLFKy4KAAAAADwlXxM3DBo0SMOHD1dKSopuvvlmSdL69ev13HPP6emnn3ZrgQAAAABQmPIVkp599lmdOXNGjz/+uJKTkyVJ/v7+GjlypEaPHu3WAgEAAACgMOX7PkmSlJCQoL179yogIEC1atWS1Wp1Z21uwX2SAAAAAEiuZ4N8XZOUoVSpUrr++usVHBysgwcPymazXUl3AAAAAOBxeQpJs2bN0ltvveXQNnjwYFWvXl2NGjVSw4YNdezYMbcWCAAAAACFKU8h6cMPP1SZMmXsz1evXq3Zs2dr7ty52rZtm0qXLq1x48a5vUgAAAAAKCx5mrjhwIEDat68uf35V199pR49eqhfv36SpNdee00DBw50b4UAAAAAUIjyNJJ04cIFhwucfvzxR7Vt29b+vHr16oqJiXFfdQAAAABQyPIUkqKiorR9+3ZJ0j///KPff/9dN9xwg315TEyMQkND3VshAAAAABSiPJ1u179/fw0ZMkS///67vv32W9WtW1fNmjWzL//xxx/VsGFDtxcJAAAAAIUlTyHpueeeU2JiohYvXqyIiAgtWrTIYfnmzZvVt29ftxYIAAAAAIXpim4mWxxwM1kAAAAAUiHdTBYAAAAAShpCEgAAAACYEJIAAAAAwISQBAAAAAAmhCQAAAAAMMnTFOB33HGH0/bQ0FDVrl1bDz/8sCpUqOCWwgAAAADAE/I0khQaGur0ERsbq5kzZ6pOnTravXu3y/1Nnz5djRs3VkhIiEJCQhQdHa1Vq1bZl1+8eFFDhgxRuXLlVKpUKfXu3VunTp3KS8kAAAAAkCduu0+SzWbToEGDdPr0aS1fvtylbZYvXy5vb2/VqlVLhmHok08+0RtvvKGdO3eqQYMGeuyxx7RixQrNmTNHoaGhGjp0qLy8vLR582aX6+I+SQAAAAAk17OBW28m++uvv6pLly46ceJEvvsoW7as3njjDd15552qUKGC5s+frzvvvFOStG/fPtWrV09btmxRq1atXOqPkAQAAABA8tDNZIOCgpSYmJivbdPS0rRgwQKdP39e0dHR2r59u1JSUtShQwf7OnXr1lWVKlW0ZcuWbPtJSkpSfHy8wwMAAAAAXOXWkLR27VrVrl07T9v89ttvKlWqlKxWqx599FEtWbJE9evXV0xMjPz8/FS6dGmH9cPDwxUTE5NtfxMmTHC4Xqpy5cr5eSsAAAAArlJ5mt1u2bJlTtvj4uK0fft2ffTRR/roo4/yVECdOnW0a9cuxcXF6csvv1T//v21adOmPPVhNnr0aI0YMcL+PD4+nqAEAAAAwGV5Ckk9e/Z02h4cHKw6deroo48+Up8+ffJUgJ+fn2rWrClJatasmbZt26Zp06bpnnvuUXJysmJjYx1Gk06dOqWIiIhs+7NarbJarXmqAQAAAAAy5Ckk2Wy2gqrD4TWSkpLUrFkz+fr6av369erdu7ckaf/+/Tp69Kiio6MLvA4AAAAAV6c8hSR3Gz16tLp06aIqVaro3Llzmj9/vjZu3Kg1a9YoNDRUDz30kEaMGKGyZcsqJCRETzzxhKKjo12e2Q4AAAAA8ipPEzd07dpVcXFx9ucTJ05UbGys/fmZM2dUv359l/s7ffq0HnjgAdWpU0e33HKLtm3bpjVr1ujWW2+VJE2ZMkW33XabevfurbZt2yoiIkKLFy/OS8kAAAAAkCd5uk+St7e3Tp48qbCwMElSSEiIdu3aperVq0tKv16oYsWKSktLK5hq84H7JAEAAACQCug+SZnzlBvvQwsAAAAARYJb75MEAAAAAMVdnkKSxWKRxWLJ0gYAAAAAJUWeZrczDEMDBgyw34fo4sWLevTRRxUUFCRJSkpKcn+FAAAAAFCI8hSS+vfv7/D8vvvuy7LOAw88cGUVAQAAAIAH5SkkzZ49u6DqAAAAAIAiwa0TN+zbt0+1a9d2Z5cAAAAAUKjcGpKSkpJ08OBBd3YJAAAAAIWKKcABAAAAwISQBAAAAAAmhCQAAAAAMMnT7HZlypTJ8eaxqampV1wQAAAAAHhSnkLS1KlTC6gMAAAAACgaruhmsgAAAABQ0rjlmqQ///xTv//+u2w2mzu6AwAAAACPyVNISk5O1pgxY9S9e3e9+uqrSktLU9++fVWrVi01btxYDRs21OHDhwuoVAAAAAAoeHkKSaNHj9b06dMVERGhWbNm6Y477tDOnTs1f/58LViwQD4+PnrhhRcKqlYAAAAAKHB5uibpyy+/1Jw5c9S1a1f973//U926dbVixQp16dJFkhQWFqZ+/foVSKEAAAAAUBjyNJJ04sQJNWnSRJJUu3ZtWa1W1axZ0768du3aiomJcW+FAAAAAFCI8hSS0tLS5Ovra3/u4+Mjb2/vy515eckwDPdVBwAAAACFLE+n20nSmjVrFBoaKkmy2Wxav369du/eLUmKjY11a3EAAAAAUNgsRh6Gfry8ch94slgsSktLu6Ki3Ck+Pl6hoaGKi4tTSEiIp8sBAAAA4CGuZoM8jSRxHyQAAAAAJZ1bbiYLAAAAACUFIQkAAAAATAhJAAAAAGBCSAIAAAAAkzyFpD///LOg6gAAAACAIiFPIalx48Zq2LChnn/+ef30008FVRMAAAAAeEyeQtI///yjCRMm6PTp0+rRo4ciIyM1aNAgLV++XBcvXiyoGgEAAACg0OTpZrJmhmFoy5YtWrZsmZYtW6ajR4+qQ4cOuv3229W9e3dVqFDB3bXmCzeTBQAAACC5ng3yPXGDxWJR69atNXHiRO3Zs0c7d+5UmzZtNGfOHFWqVEnvvfdefrsGAAAAAI/J90hSTs6cOaN///1XtWrVcnfXecZIEgAAAADJ9WzgUxAvXq5cOZUrV64gugYAAACAAsV9kgAAAADAhJAEAAAAACaEJAAAAAAwyVdIOnbsmI4fP25//vPPP2v48OH68MMP3VYYAAAAAHhCvkLSvffeqw0bNkiSYmJidOutt+rnn3/WCy+8oPHjx7u1QAAAAAAoTPkKSbt371aLFi0kSV988YUaNmyoH3/8UfPmzdOcOXPcWR8AAAAAFKp8haSUlBRZrVZJ0rp163T77bdLkurWrauTJ0+6rzoAAAAAKGT5CkkNGjTQjBkz9P3332vt2rXq3LmzJOnEiRPcHwkAAABAsZavkDRp0iR98MEHat++vfr27asmTZpIkpYtW2Y/DQ8AAAAAiiOLYRhGfjZMS0tTfHy8ypQpY287fPiwAgMDFRYW5rYCr1R8fLxCQ0MVFxenkJAQT5cDAAAAwENczQb5Gkm6cOGCkpKS7AHpyJEjmjp1qvbv31+kAhIAAAAA5FW+QlKPHj00d+5cSVJsbKxatmypN998Uz179tT06dPdWiAAAAAAFKZ8haQdO3aoTZs2kqQvv/xS4eHhOnLkiObOnau3337brQUCAAAAQGHKV0hKTExUcHCwJOmbb77RHXfcIS8vL7Vq1UpHjhxxa4EAAAAAUJjyFZJq1qyppUuX6tixY1qzZo06duwoSTp9+jSTIwAAAAAo1vIVkl566SU988wzqlq1qlq0aKHo6GhJ6aNKTZs2dWuBAAAAAFCY8j0FeExMjE6ePKkmTZrIyys9a/38888KCQlR3bp13VrklWAKcAAAAACS69nAJ78vEBERoYiICB0/flySVKlSJW4kCwAAAKDYy9fpdjabTePHj1doaKiioqIUFRWl0qVL6+WXX5bNZnN3jQAAAABQaPI1kvTCCy/o448/1sSJE3XDDTdIkn744QeNHTtWFy9e1KuvvurWIgEAAACgsOTrmqSKFStqxowZuv322x3av/rqKz3++OP666+/3FbgleKaJAAAAACS69kgX6fb/fvvv04nZ6hbt67+/fff/HQJAAAAAEVCvkJSkyZN9O6772Zpf/fdd9WkSZMrLgoAAAAAPCVf1yS9/vrr6tatm9atW2e/R9KWLVt07NgxrVy50q0FAgAAAEBhytdIUrt27fS///1PvXr1UmxsrGJjY3XHHXdo//79atOmjbtrBAAAAIBCk++byTpz/PhxjR8/Xh9++KG7urxiTNwAAAAAQCrgiRuyc+bMGX388cfu7BIAAAAACpVbQxIAAAAAFHeEJAAAAAAwISQBAAAAgEmepgC/4447clweGxubpxefMGGCFi9erH379ikgIECtW7fWpEmTVKdOHfs6Fy9e1NNPP60FCxYoKSlJnTp10vvvv6/w8PA8vRYAAAAAuCJPI0mhoaE5PqKiovTAAw+43N+mTZs0ZMgQbd26VWvXrlVKSoo6duyo8+fP29d56qmntHz5ci1atEibNm3SiRMncg1rAAAAAJBfbp0C/Er9/fffCgsL06ZNm9S2bVvFxcWpQoUKmj9/vu68805J0r59+1SvXj1t2bJFrVq1yrVPpgAHAAAAIHloCvArFRcXJ0kqW7asJGn79u1KSUlRhw4d7OvUrVtXVapU0ZYtW5z2kZSUpPj4eIcHAAAAALiqyIQkm82m4cOH64YbblDDhg0lSTExMfLz81Pp0qUd1g0PD1dMTIzTfiZMmOBwCmDlypULunQAAAAAJUiRCUlDhgzR7t27tWDBgivqZ/To0YqLi7M/jh075qYKAQAAAFwN8jS7XUEZOnSovv76a3333XeqVKmSvT0iIkLJycmKjY11GE06deqUIiIinPZltVpltVoLumQAAAAAJZRHR5IMw9DQoUO1ZMkSffvtt6pWrZrD8mbNmsnX11fr16+3t+3fv19Hjx5VdHR0YZcLAAAA4Crg0ZGkIUOGaP78+frqq68UHBxsv84oNDRUAQEBCg0N1UMPPaQRI0aobNmyCgkJ0RNPPKHo6GiXZrYDAAAAgLzy6BTgFovFafvs2bM1YMAASZdvJvv555873Ew2u9PtMmMKcAAAAACS69mgSN0nqSAQkgAAAABIxfQ+SQAAAADgaYQkAAAAADAhJAEAAACACSEJAAAAAEwISQAAAABgQkgCAAAAABNCEgAAAACYEJIAAAAAwISQBAAAAAAmhCQAAAAAMCEkAQAAAIAJIQkAAAAATAhJAAAAAGBCSAIAAAAAE0ISAAAAAJgQkgAAAADAhJAEAAAAACaEJAAAAAAwISQBAAAAgAkhCQAAAABMCEkAAAAAYEJIAgAAAAATQhIAAAAAmBCSAAAAAMCEkAQAAAAAJoQkAAAAADAhJAEAAACACSEJAAAAAEwISQAAAABgQkgCAAAAABNCEgAAAACYEJIAAAAAwISQBAAAAAAmhCQAAAAAMCEkAQAAAIAJIQkAAAAATAhJAAAAAGBCSAIAAAAAE0ISAAAAAJgQkgAAAADAhJAEAAAAACaEJAAAAAAwISQBAAAAgAkhCQAAAABMCEkAAAAAYEJIAgAAAAATQhIAAAAAmBCSAAAAAMCEkAQAAAAAJoQkAAAAADAhJAEAAACACSEJAAAAAEwISQAAAABgQkgCAAAAABNCEgAAAACYEJIAAAAAwISQBAAAAAAmhCQAAAAAMCEkAQAAAIAJIQkAAAAATAhJAAAAAGBCSAIAAAAAE0ISAAAAAJgQkgAAAADAxKMh6bvvvlP37t1VsWJFWSwWLV261GG5YRh66aWXFBkZqYCAAHXo0EEHDhzwTLEAAAAArgoeDUnnz59XkyZN9N577zld/vrrr+vtt9/WjBkz9NNPPykoKEidOnXSxYsXC7lSAAAAAFcLH0++eJcuXdSlSxenywzD0NSpU/Wf//xHPXr0kCTNnTtX4eHhWrp0qfr06VOYpQIAAAC4ShTZa5IOHTqkmJgYdejQwd4WGhqqli1basuWLdlul5SUpPj4eIcHAAAAALiqyIakmJgYSVJ4eLhDe3h4uH2ZMxMmTFBoaKj9Ubly5QKtEwAAAEDJUmRDUn6NHj1acXFx9sexY8c8XRIAAACAYqTIhqSIiAhJ0qlTpxzaT506ZV/mjNVqVUhIiMMDAAAAAFxVZENStWrVFBERofXr19vb4uPj9dNPPyk6OtqDlQEAAAAoyTw6u11CQoL++OMP+/NDhw5p165dKlu2rKpUqaLhw4frlVdeUa1atVStWjW9+OKLqlixonr27Om5ogEAAACUaB4NSb/88otuuukm+/MRI0ZIkvr37685c+boueee0/nz5zV48GDFxsbqxhtv1OrVq+Xv7++pkgEAAACUcBbDMAxPF1GQ4uPjFRoaqri4OK5PAgAAAK5irmaDIntNEgAAAAB4AiEJAAAAAEwISQAAAABgQkgCAAAAABNCEgAAAACYEJIAAAAAwISQBAAAAAAmhCQAAAAAMCEkAQAAAIAJIQkAAAAATAhJAAAAAGBCSAIAAAAAE0ISAAAAAJgQkgAAAADAhJAEAAAAACaEJAAAAAAwISQBAAAAgAkhCQAAAABMCEkAAAAAYEJIAgAAAAATQhIAAAAAmBCSAAAAAMCEkAQAAAAAJoQkAAAAADAhJAEAAACACSEJAAAAAEwISQAAAABgQkgCAAAAABNCEgAAAACYEJIAAAAAwISQBAAAAAAmhCQAAAAAMCEkAQAAAIAJIQkAAAAATAhJAAAAAGBCSAIAAAAAE0ISAAAAAJgQkgAAAADAhJAEAAAAACaEJAAAAAAwISQBAAAAgAkhCQAAAABMCEkAAAAAYEJIAgAAAAATQhIAAAAAmBCSAAAAAMCEkAQAAAAAJoQkAAAAADAhJAEAAACACSEJAAAAAEwISQAAAABgQkgCAAAAABNCEgAAAACYEJIAAAAAwMTH0wVcTb7dd0qpaYZ8fbzk6+UlH2+LfL0t8vX2ko+X1+Wv7W0Wh3V9vCyyWCyefhsAAABAiUZIKkTPfflf/ZOQfEV9+Hpb5ON1OUhlPPf1tsjH28vUlv7czzsjYGUKYV5e8vVxsq1XxteO62Z+PR9vy6W+c+ov87Ze8vYi5AEAAKBoIyQVosaVSutsYrJS0mxKTTPS/7UZSkm1KcVmKDXNphRTe5rNyNJH+vI0KcUDb8ANLBalBypTkPK5FLAuj5h5XRpBszgELB8vx5G2rIEtcxg0fZ3rtpfXMQdEHy9T0LwU+rwIegAAACUaIakQzRpwfZ7Wt9kMpdjSA1VqWvrXmQNWcmr6v44BK/3rjPXsYSwto928bUZ/mbd1DGz2ftIMpVwKdvZtbTalpBpZt7XZZGTKeYYhJafZlJwmSWlu27eFydvLkjV0ORmBcwxY6es4P53SYh+R87t06qXDqZjO+s90Kmbm0zYzr+s4qshpmwAAADkhJBVhXl4WWb28ZS3Gn1JaloBlM4U3wx60ktMywqDt8tc2U1Bztm1a+ghc+vLL/aWkmsKlzabk1MyvkxH8nGxrer2M587eU5rNUFKqzQN71D0yhzzzKFrGqJqfj7OAlcupmA5h8PKpnw7X1nl7yS/LaaM59MdpmwAAoJAV41+/URx4e1nk7eUtf19vT5eSL4ZhXBqpyxgxuzyyljnsXQ5vOax7adTPIRg6hDObaaQul22zCY2unLaZajOUaiv+p226fupl5jBoGu3zce20zcz9Zd7W2WmhOYVLTtsEAKDoIiQBObBYMk5lkwJUPINe5tM2nQWsjECV40ieC6dturYtp21KOZ+2mWNgc3Ktnn10zjyRir3fzNf5Zb1WL8u6OYwqmrfltE0AQElFSAJKuJJ62qY5SGU5zTKX0zYvj9Rl9OFkFC7X00Odn7ZpDoTmgOjsPZWE0zadjaLlHrByOp3SIsfr8LKZsdPJdX65nQbq4+2ljFiXke8ssiij8XJbxnOL/bnD+qbnUuZtLTn247AuIRMAiqxi/GsTgKtFSTpt0zFgOQl7OZy2mfW0ysvrptpsDtf25XbaZk7XBaY4CXnZn7Zp6GJK8Q16RUVeApqyrHt5eW4BzZQJLy9z4bWl7NbN/bXN7zG7sOm0XhfCZvah1cm6uby2nO7z3F87p88jt9c215t1++xfO0+fR67Hj+uvLSf73JXXzqne3I4FZ+8h29c2vRe5sE3ejoXcX9uVel05FrKum2mZk3pzOv6ye+2cvseyO37M9Wb7B6BsXrt8KauaVy2r4oKQBAAF7Go7bdM86pd58hTH6/hyCWxOTtt0NlLn0rZOTtssSjJqMzI3OK5VSNUAgPu1q11BnzzYwtNluIyQBADIVUk5bVNKH9mT0iPH5XCSscxxG/PyzEHG3I/DtkbW/nLaJmNdZVnXhdd2Um9eXlvZrnv5te39FsS+crpu9tu4sq8ur+u8H+X43q6g3hxeWzmtm8trm+t15bXty7N5D873ax7qzeW1L2/rpnqdrJt5WdbjIZ/15uG1s3zv5GFfOas367qZ3ovpZbPUlZ96nfSf3TJ7D86OxZw+00z11g4vpeKkWPx399577+mNN95QTEyMmjRponfeeUctWhSfJAoA8LzLU8dbclwPAAAvTxeQm4ULF2rEiBEaM2aMduzYoSZNmqhTp046ffq0p0sDAAAAUAIV+ZD01ltvadCgQRo4cKDq16+vGTNmKDAwULNmzfJ0aQAAAABKoCIdkpKTk7V9+3Z16NDB3ubl5aUOHTpoy5YtTrdJSkpSfHy8wwMAAAAAXFWkQ9I///yjtLQ0hYeHO7SHh4crJibG6TYTJkxQaGio/VG5cuXCKBUAAABACVGkQ1J+jB49WnFxcfbHsWPHPF0SAAAAgGKkSM9uV758eXl7e+vUqVMO7adOnVJERITTbaxWq6xWa2GUBwAAAKAEKtIjSX5+fmrWrJnWr19vb7PZbFq/fr2io6M9WBkAAACAkqpIjyRJ0ogRI9S/f381b95cLVq00NSpU3X+/HkNHDjQ06UBAAAAKIGKfEi655579Pfff+ull15STEyMrr32Wq1evTrLZA4AAAAA4A4WwzAMTxdRkOLj4xUaGqq4uDiFhIR4uhwAAAAAHuJqNijS1yQBAAAAQGEjJAEAAACACSEJAAAAAEwISQAAAABgQkgCAAAAABNCEgAAAACYEJIAAAAAwISQBAAAAAAmPp4uoKBl3Cs3Pj7ew5UAAAAA8KSMTJCREbJT4kPSuXPnJEmVK1f2cCUAAAAAioJz584pNDQ02+UWI7cYVczZbDadOHFCwcHBslgsHq0lPj5elStX1rFjxxQSEuLRWkoi9m/BYv8WLPZvwWL/Fiz2b8FjHxcs9m/BKkr71zAMnTt3ThUrVpSXV/ZXHpX4kSQvLy9VqlTJ02U4CAkJ8fgBUpKxfwsW+7dgsX8LFvu3YLF/Cx77uGCxfwtWUdm/OY0gZWDiBgAAAAAwISQBAAAAgAkhqRBZrVaNGTNGVqvV06WUSOzfgsX+LVjs34LF/i1Y7N+Cxz4uWOzfglUc92+Jn7gBAAAAAPKCkSQAAAAAMCEkAQAAAIAJIQkAAAAATAhJAAAAAGBCSMqn7777Tt27d1fFihVlsVi0dOnSXLfZuHGjrrvuOlmtVtWsWVNz5szJss57772nqlWryt/fXy1bttTPP//s/uKLgbzu38WLF+vWW29VhQoVFBISoujoaK1Zs8ZhnbFjx8pisTg86tatW4DvoujK6/7duHFjln1nsVgUExPjsB7H72V53ccDBgxwuo8bNGhgX4djON2ECRN0/fXXKzg4WGFhYerZs6f279+f63aLFi1S3bp15e/vr0aNGmnlypUOyw3D0EsvvaTIyEgFBASoQ4cOOnDgQEG9jSIrP/t35syZatOmjcqUKaMyZcqoQ4cOWb7/nR3jnTt3Lsi3UiTlZ//OmTMny77z9/d3WIfjN11+9m/79u2d/vzt1q2bfR2O33TTp09X48aN7TeFjY6O1qpVq3Lcprj+7CUk5dP58+fVpEkTvffeey6tf+jQIXXr1k033XSTdu3apeHDh+vhhx92+EV+4cKFGjFihMaMGaMdO3aoSZMm6tSpk06fPl1Qb6PIyuv+/e6773Trrbdq5cqV2r59u2666SZ1795dO3fudFivQYMGOnnypP3xww8/FET5RV5e92+G/fv3O+y/sLAw+zKOX0d53cfTpk1z2LfHjh1T2bJldddddzmsxzEsbdq0SUOGDNHWrVu1du1apaSkqGPHjjp//ny22/z444/q27evHnroIe3cuVM9e/ZUz549tXv3bvs6r7/+ut5++23NmDFDP/30k4KCgtSpUyddvHixMN5WkZGf/btx40b17dtXGzZs0JYtW1S5cmV17NhRf/31l8N6nTt3djh+P//884J+O0VOfvavJIWEhDjsuyNHjjgs5/hNl5/9u3jxYod9u3v3bnl7e2f5+cvxK1WqVEkTJ07U9u3b9csvv+jmm29Wjx499Pvvvztdv1j/7DVwxSQZS5YsyXGd5557zmjQoIFD2z333GN06tTJ/rxFixbGkCFD7M/T0tKMihUrGhMmTHBrvcWNK/vXmfr16xvjxo2zPx8zZozRpEkT9xVWQriyfzds2GBIMs6ePZvtOhy/2cvPMbxkyRLDYrEYhw8ftrdxDDt3+vRpQ5KxadOmbNe5++67jW7dujm0tWzZ0njkkUcMwzAMm81mREREGG+88YZ9eWxsrGG1Wo3PP/+8YAovJlzZv5mlpqYawcHBxieffGJv69+/v9GjR48CqLB4c2X/zp492wgNDc12Ocdv9vJz/E6ZMsUIDg42EhIS7G0cv9krU6aM8dFHHzldVpx/9jKSVEi2bNmiDh06OLR16tRJW7ZskSQlJydr+/btDut4eXmpQ4cO9nXgOpvNpnPnzqls2bIO7QcOHFDFihVVvXp19evXT0ePHvVQhcXTtddeq8jISN16663avHmzvZ3j1/0+/vhjdejQQVFRUQ7tHMNZxcXFSVKW73ez3H4GHzp0SDExMQ7rhIaGqmXLllf9MezK/s0sMTFRKSkpWbbZuHGjwsLCVKdOHT322GM6c+aMW2stjlzdvwkJCYqKilLlypWz/OWe4zd7+Tl+P/74Y/Xp00dBQUEO7Ry/jtLS0rRgwQKdP39e0dHRTtcpzj97CUmFJCYmRuHh4Q5t4eHhio+P14ULF/TPP/8oLS3N6TqZr/tA7iZPnqyEhATdfffd9raWLVtqzpw5Wr16taZPn65Dhw6pTZs2OnfunAcrLR4iIyM1Y8YM/d///Z/+7//+T5UrV1b79u21Y8cOSeL4dbMTJ05o1apVevjhhx3aOYazstlsGj58uG644QY1bNgw2/Wy+xmccXxm/Msx7MjV/ZvZyJEjVbFiRYdffDp37qy5c+dq/fr1mjRpkjZt2qQuXbooLS2tIEovFlzdv3Xq1NGsWbP01Vdf6bPPPpPNZlPr1q11/PhxSRy/2cnP8fvzzz9r9+7dWX7+cvxe9ttvv6lUqVKyWq169NFHtWTJEtWvX9/pusX5Z6+PR18dKADz58/XuHHj9NVXXzlcM9OlSxf7140bN1bLli0VFRWlL774Qg899JAnSi026tSpozp16tift27dWgcPHtSUKVP06aeferCykumTTz5R6dKl1bNnT4d2juGshgwZot27d1+V12YVhvzs34kTJ2rBggXauHGjw+QCffr0sX/dqFEjNW7cWDVq1NDGjRt1yy23uLXu4sLV/RsdHe3wl/rWrVurXr16+uCDD/Tyyy8XdJnFVn6O348//liNGjVSixYtHNo5fi+rU6eOdu3apbi4OH355Zfq37+/Nm3alG1QKq4YSSokEREROnXqlEPbqVOnFBISooCAAJUvX17e3t5O14mIiCjMUou1BQsW6OGHH9YXX3yRZXg3s9KlS6t27dr6448/Cqm6kqVFixb2fcfx6z6GYWjWrFm6//775efnl+O6V/sxPHToUH399dfasGGDKlWqlOO62f0Mzjg+M/7lGL4sL/s3w+TJkzVx4kR98803aty4cY7rVq9eXeXLl+f4zcP+zeDr66umTZva9x3Hb1b52b/nz5/XggULXPqj09V8/Pr5+almzZpq1qyZJkyYoCZNmmjatGlO1y3OP3sJSYUkOjpa69evd2hbu3at/S9Dfn5+atasmcM6NptN69evz/Y8Tzj6/PPPNXDgQH3++ecO03ZmJyEhQQcPHlRkZGQhVFfy7Nq1y77vOH7dZ9OmTfrjjz9c+k/6aj2GDcPQ0KFDtWTJEn377beqVq1artvk9jO4WrVqioiIcFgnPj5eP/3001V3DOdn/0rpM1S9/PLLWr16tZo3b57r+sePH9eZM2c4fl3cv2ZpaWn67bff7PuO4/eyK9m/ixYtUlJSku67775c171aj19nbDabkpKSnC4r1j97PTptRDF27tw5Y+fOncbOnTsNScZbb71l7Ny50zhy5IhhGIYxatQo4/7777ev/+effxqBgYHGs88+a+zdu9d47733DG9vb2P16tX2dRYsWGBYrVZjzpw5xp49e4zBgwcbpUuXNmJiYgr9/XlaXvfvvHnzDB8fH+O9994zTp48aX/Exsba13n66aeNjRs3GocOHTI2b95sdOjQwShfvrxx+vTpQn9/npbX/TtlyhRj6dKlxoEDB4zffvvNGDZsmOHl5WWsW7fOvg7Hr6O87uMM9913n9GyZUunfXIMp3vssceM0NBQY+PGjQ7f74mJifZ17r//fmPUqFH255s3bzZ8fHyMyZMnG3v37jXGjBlj+Pr6Gr/99pt9nYkTJxqlS5c2vvrqK+O///2v0aNHD6NatWrGhQsXCvX9eVp+9u/EiRMNPz8/48svv3TY5ty5c4ZhpH8/PPPMM8aWLVuMQ4cOGevWrTOuu+46o1atWsbFixcL/T16Un7277hx44w1a9YYBw8eNLZv32706dPH8Pf3N37//Xf7Ohy/6fKzfzPceOONxj333JOlneP3slGjRhmbNm0yDh06ZPz3v/81Ro0aZVgsFuObb74xDKNk/ewlJOVTxpTImR/9+/c3DCN9qsh27dpl2ebaa681/Pz8jOrVqxuzZ8/O0u8777xjVKlSxfDz8zNatGhhbN26teDfTBGU1/3brl27HNc3jPQp1yMjIw0/Pz/jmmuuMe655x7jjz/+KNw3VkTkdf9OmjTJqFGjhuHv72+ULVvWaN++vfHtt99m6Zfj97L8/IyIjY01AgICjA8//NBpnxzD6ZztV0kOP1PbtWvn8P1vGIbxxRdfGLVr1zb8/PyMBg0aGCtWrHBYbrPZjBdffNEIDw83rFarccsttxj79+8vhHdUtORn/0ZFRTndZsyYMYZhGEZiYqLRsWNHo0KFCoavr68RFRVlDBo06Kr8I0p+9u/w4cPtP1vDw8ONrl27Gjt27HDol+M3XX5/Puzbt8+QZP9l34zj97IHH3zQiIqKMvz8/IwKFSoYt9xyi8M+K0k/ey2GYRhuGpQCAAAAgGKPa5IAAAAAwISQBAAAAAAmhCQAAAAAMCEkAQAAAIAJIQkAAAAATAhJAAAAAGBCSAIAAAAAE0ISAAAAAJgQkgAAyIHFYtHSpUs9XQYAoBARkgAARdaAAQNksViyPDp37uzp0gAAJZiPpwsAACAnnTt31uzZsx3arFarh6oBAFwNGEkCABRpVqtVERERDo8yZcpISj8Vbvr06erSpYsCAgJUvXp1ffnllw7b//bbb7r55psVEBCgcuXKafDgwUpISHBYZ9asWWrQoIGsVqsiIyM1dOhQh+X//POPevXqpcDAQNWqVUvLli0r2DcNAPAoQhIAoFh78cUX1bt3b/3666/q16+f+vTpo71790qSzp8/r06dOqlMmTLatm2bFi1apHXr1jmEoOnTp2vIkCEaPHiwfvvtNy1btkw1a9Z0eI1x48bp7rvv1n//+1917dpV/fr107///luo7xMAUHgshmEYni4CAABnBgwYoM8++0z+/v4O7c8//7yef/55WSwWPfroo5o+fbp9WatWrXTdddfp/fff18yZMzVy5EgdO3ZMQUFBkqSVK1eqe/fuOnHihMLDw3XNNddo4MCBeuWVV5zWYLFY9J///Ecvv/yypPTgVapUKa1atYprowCghOKaJABAkXbTTTc5hCBJKlu2rP3r6Ohoh2XR0dHatWuXJGnv3r1q0qSJPSBJ0g033CCbzab9+/fLYrHoxIkTuuWWW3KsoXHjxvavg4KCFBISotOnT+f3LQEAijhCEgCgSAsKCspy+pu7BAQEuLSer6+vw3OLxSKbzVYQJQEAigCuSQIAFGtbt27N8rxevXqSpHr16unXX3/V+fPn7cs3b94sLy8v1alTR8HBwapatarWr19fqDUDAIo2RpIAAEVaUlKSYmJiHNp8fHxUvnx5SdKiRYvUvHlz3XjjjZo3b55+/vlnffzxx5Kkfv36acyYMerfv7/Gjh2rv//+W0888YTuv/9+hYeHS5LGjh2rRx99VGFhYerSpYvOnTunzZs364knnijcNwoAKDIISQCAIm316tWKjIx0aKtTp4727dsnKX3muQULFujxxx9XZGSkPv/8c9WvX1+SFBgYqDVr1mjYsGG6/vrrFRgYqN69e+utt96y99W/f39dvHhRU6ZM0TPPPKPy5cvrzjvvLLw3CAAocpjdDgBQbFksFi1ZskQ9e/b0dCkAgBKEa5IAAAAAwISQBAAAAAAmXJMEACi2OGMcAFAQGEkCAAAAABNCEgAAAACYEJIAAAAAwISQBAAAAAAmhCQAAAAAMCEkAQAAAIAJIQkAAAAATAhJAAAAAGDy/5oUmIkZSuzyAAAAAElFTkSuQmCC"},"metadata":{}},{"name":"stdout","text":"Generating predictions on the test set...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_30/3280749432.py:238: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"name":"stdout","text":"Computing BLEU scores for each test sample...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x600 with 1 Axes>","image/png":"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"},"metadata":{}},{"name":"stdout","text":"Average BLEU score on test set: 32.22\n\nSample translations from the test set:\n\nSource: delete all recurring alarms\nReference: sarey recurring alarms delete karo\nPrediction: sabhi recurring alarms delete karo\n\nSource: what is the best route to the racetrack from my sister ' s house , if i left around 2 pm ?\nReference: agar mai 2 pm ke aas paas nikal jaoon to meri sister ke ghar se racetrack tak pahunchne ke liye sabse best route konsa hai ?\nPrediction: agar mai 2 pm ke aas paas nikal jaoon to meri sister ke ghar se racetrack tak sabse acha rasta kaunsa hai ?\n\nSource: play the top 40 songs right now\nReference: top 40 songs ko abhi play kare\nPrediction: top 40 songs abhi bajao\n\nSource: i want to cancel that alarm\nReference: mai wo alarm cancel karna chahta hoon\nPrediction: mai wo alarm cancel karna chahta hoon\n\nSource: alert me at 6 tomorrow instead of 7 am .\nReference: mujhe kal 7 am ke bajaye 6 ko alert kare\nPrediction: mujhe kal subah 7 baje ke bajaye 6 baje alert karen .\n\nCustom Translation:\nInput: I was waiting for my bag\nTranslated Output: main apne bag ko wait kar raha tha\nAllocated GPU memory: 3.39 GB\nCached GPU memory: 8.29 GB\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_30/3280749432.py:290: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"custom_sentence = \"what are you doing?\"\ntranslated_sentence = translate_sentence(custom_sentence)\nprint(\"\\nCustom Translation:\")\nprint(f\"Input: {custom_sentence}\")\nprint(f\"Translated Output: {translated_sentence}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T20:20:34.272039Z","iopub.execute_input":"2024-11-12T20:20:34.272482Z","iopub.status.idle":"2024-11-12T20:20:34.500430Z","shell.execute_reply.started":"2024-11-12T20:20:34.272442Z","shell.execute_reply":"2024-11-12T20:20:34.498325Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/3280749432.py:290: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"name":"stdout","text":"\nCustom Translation:\nInput: what are you doing?\nTranslated Output: tumhe kya kar raha hai?\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"custom_sentence = \"i should wakeup early tommorow\"\ntranslated_sentence = translate_sentence(custom_sentence)\nprint(\"\\nCustom Translation:\")\nprint(f\"Input: {custom_sentence}\")\nprint(f\"Translated Output: {translated_sentence}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T20:21:10.853187Z","iopub.execute_input":"2024-11-12T20:21:10.853555Z","iopub.status.idle":"2024-11-12T20:21:11.095621Z","shell.execute_reply.started":"2024-11-12T20:21:10.853520Z","shell.execute_reply":"2024-11-12T20:21:11.094661Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/3280749432.py:290: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"name":"stdout","text":"\nCustom Translation:\nInput: i should wakeup early tommorow\nTranslated Output: mujhe kal jaldi jagaane ki zaroorat hai\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"custom_sentence = \"i want to eat pizza\"\ntranslated_sentence = translate_sentence(custom_sentence)\nprint(\"\\nCustom Translation:\")\nprint(f\"Input: {custom_sentence}\")\nprint(f\"Translated Output: {translated_sentence}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T20:22:44.864130Z","iopub.execute_input":"2024-11-12T20:22:44.865133Z","iopub.status.idle":"2024-11-12T20:22:45.030996Z","shell.execute_reply.started":"2024-11-12T20:22:44.865078Z","shell.execute_reply":"2024-11-12T20:22:45.030000Z"}},"outputs":[{"name":"stdout","text":"\nCustom Translation:\nInput: i want to eat pizza\nTranslated Output: mai pizza khaana chahta hoon\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_30/3280749432.py:290: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"custom_sentence = \"i will not go to college tommorow\"\ntranslated_sentence = translate_sentence(custom_sentence)\nprint(\"\\nCustom Translation:\")\nprint(f\"Input: {custom_sentence}\")\nprint(f\"Translated Output: {translated_sentence}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T20:24:09.526020Z","iopub.execute_input":"2024-11-12T20:24:09.526433Z","iopub.status.idle":"2024-11-12T20:24:09.732616Z","shell.execute_reply.started":"2024-11-12T20:24:09.526393Z","shell.execute_reply":"2024-11-12T20:24:09.731187Z"}},"outputs":[{"name":"stdout","text":"\nCustom Translation:\nInput: i will not go to college tommorow\nTranslated Output: mai kal college me nahi jaunga\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_30/3280749432.py:290: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"custom_sentence = \"love me please, siri\"\ntranslated_sentence = translate_sentence(custom_sentence)\nprint(\"\\nCustom Translation:\")\nprint(f\"Input: {custom_sentence}\")\nprint(f\"Translated Output: {translated_sentence}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T20:26:15.778290Z","iopub.execute_input":"2024-11-12T20:26:15.778685Z","iopub.status.idle":"2024-11-12T20:26:15.950954Z","shell.execute_reply.started":"2024-11-12T20:26:15.778646Z","shell.execute_reply":"2024-11-12T20:26:15.947503Z"}},"outputs":[{"name":"stdout","text":"\nCustom Translation:\nInput: love me please, siri\nTranslated Output: please mujhe love karo, siri\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_30/3280749432.py:290: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"custom_sentence = \"nowadays there are a lot of people here\"\ntranslated_sentence = translate_sentence(custom_sentence)\nprint(\"\\nCustom Translation:\")\nprint(f\"Input: {custom_sentence}\")\nprint(f\"Translated Output: {translated_sentence}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T20:27:37.704562Z","iopub.execute_input":"2024-11-12T20:27:37.704976Z","iopub.status.idle":"2024-11-12T20:27:37.891590Z","shell.execute_reply.started":"2024-11-12T20:27:37.704938Z","shell.execute_reply":"2024-11-12T20:27:37.890538Z"}},"outputs":[{"name":"stdout","text":"\nCustom Translation:\nInput: nowadays there are a lot of people here\nTranslated Output: aaj raat yaha bahut log hai\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_30/3280749432.py:290: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"custom_sentence = \"i will drink perier instead of water\"\ntranslated_sentence = translate_sentence(custom_sentence)\nprint(\"\\nCustom Translation:\")\nprint(f\"Input: {custom_sentence}\")\nprint(f\"Translated Output: {translated_sentence}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T20:39:43.552712Z","iopub.execute_input":"2024-11-12T20:39:43.553750Z","iopub.status.idle":"2024-11-12T20:39:43.776102Z","shell.execute_reply.started":"2024-11-12T20:39:43.553708Z","shell.execute_reply":"2024-11-12T20:39:43.775088Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/3280749432.py:290: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"name":"stdout","text":"\nCustom Translation:\nInput: i will drink perier instead of water\nTranslated Output: mai water ke bajaye perier ko drink kar raha hoon\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"custom_sentence = \"The bright sun shines warmly on the beach\"\ntranslated_sentence = translate_sentence(custom_sentence)\nprint(\"\\nCustom Translation:\")\nprint(f\"Input: {custom_sentence}\")\nprint(f\"Translated Output: {translated_sentence}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T20:43:04.522365Z","iopub.execute_input":"2024-11-12T20:43:04.523100Z","iopub.status.idle":"2024-11-12T20:43:04.692969Z","shell.execute_reply.started":"2024-11-12T20:43:04.523059Z","shell.execute_reply":"2024-11-12T20:43:04.691959Z"}},"outputs":[{"name":"stdout","text":"\nCustom Translation:\nInput: The bright sun shines warmly on the beach\nTranslated Output: beach par bright sun shuru hota hai\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_30/3280749432.py:290: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"# Save the model as a .pt file\nmodel_save_path = \"Opus-HinglishMT.pt\"\ntorch.save(model.state_dict(), model_save_path)\nprint(f\"Model saved as {model_save_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T20:46:19.606530Z","iopub.execute_input":"2024-11-12T20:46:19.607397Z","iopub.status.idle":"2024-11-12T20:46:20.156064Z","shell.execute_reply.started":"2024-11-12T20:46:19.607351Z","shell.execute_reply":"2024-11-12T20:46:20.155102Z"}},"outputs":[{"name":"stdout","text":"Model saved as Opus-HinglishMT.pt\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"# Save the model after training\nmodel_save_path = \"opus_mt_model_state_dict.pth\"\n\n# Check if model is wrapped in DataParallel\nif isinstance(model, torch.nn.DataParallel):\n    # If wrapped in DataParallel, save the state_dict of the `module` (the original model)\n    torch.save(model.module.state_dict(), model_save_path)\nelse:\n    # If not wrapped in DataParallel, save the state_dict directly\n    torch.save(model.state_dict(), model_save_path)\n\nprint(f\"Model saved to {model_save_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T20:50:12.347070Z","iopub.execute_input":"2024-11-12T20:50:12.348097Z","iopub.status.idle":"2024-11-12T20:50:12.746974Z","shell.execute_reply.started":"2024-11-12T20:50:12.348019Z","shell.execute_reply":"2024-11-12T20:50:12.745869Z"}},"outputs":[{"name":"stdout","text":"Model saved to opus_mt_model_state_dict.pth\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"# Load the model for testing or future use\nloaded_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, config=config)\nloaded_model.load_state_dict(torch.load(model_save_path, map_location=device))\nloaded_model.to(device)\n\n# Wrap in DataParallel if multiple GPUs are available\nif n_gpu > 1:\n    loaded_model = torch.nn.DataParallel(loaded_model)\n\nprint(\"Model loaded successfully.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T20:50:29.770269Z","iopub.execute_input":"2024-11-12T20:50:29.770803Z","iopub.status.idle":"2024-11-12T20:50:33.240374Z","shell.execute_reply.started":"2024-11-12T20:50:29.770745Z","shell.execute_reply":"2024-11-12T20:50:33.239307Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/3401000727.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  loaded_model.load_state_dict(torch.load(model_save_path, map_location=device))\n","output_type":"stream"},{"name":"stdout","text":"Model loaded successfully.\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}